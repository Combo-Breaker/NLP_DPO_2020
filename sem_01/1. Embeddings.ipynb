{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Векторные модели, которые мы рассматривали до этого, условно называются *счётными*. Они основываются на том, что так или иначе \"считают\" слова и их соседей, и на основе этого строят вектора для слов. \n",
    "\n",
    "Другой класс моделей, который более повсевмёстно распространён на сегодняшний день, называется *предсказательными* (или *нейронными*) моделями. Идея этих моделей заключается в использовании нейросетевых архитектур, которые \"предсказывают\" (а не считают) соседей слов. Одной из самых известных таких моделей является word2vec. Технология основана на нейронной сети, предсказывающей вероятность встретить слово в заданном контексте. Этот инструмент был разработан группой исследователей Google в 2013 году, руководителем проекта был Томаш Миколов (сейчас работает в Facebook). Вот две самые главные статьи:\n",
    "\n",
    "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "\n",
    "$$\\hat{P}(w_1^T) = \\prod_{t=1}^T \\hat{P}(w_t \\mid w_1^{t-1})$$\n",
    "\n",
    "Полученные таким образом вектора называются *распределенными представлениями слов*, или **эмбеддингами**.\n",
    "\n",
    "### Зачем это нужно?\n",
    "\n",
    "* Решать лингвистические задачи (в основном это про семантику и сочетаемость)\n",
    "* Подавать на вход нейронным сетям\n",
    "\n",
    "### Как это обучается?\n",
    "Мы задаём вектор для каждого слова с помощью матрицы $w$ и вектор контекста с помощью матрицы $W$. По сути, word2vec является обобщающим названием для двух архитектур Skip-Gram и Continuous Bag-Of-Words (CBOW).  \n",
    "\n",
    "**CBOW** предсказывает текущее слово, исходя из окружающего его контекста. \n",
    "\n",
    "**Skip-gram**, наоборот, использует текущее слово, чтобы предугадывать окружающие его слова. \n",
    "\n",
    "### Как это работает?\n",
    "Word2vec принимает большой текстовый корпус в качестве входных данных и сопоставляет каждому слову вектор, выдавая координаты слов на выходе. Сначала он создает словарь, «обучаясь» на входных текстовых данных, а затем вычисляет векторное представление слов. Векторное представление основывается на контекстной близости: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, согласно дистрибутивной гипотезе, имеющие схожий смысл), в векторном представлении будут иметь близкие координаты векторов-слов. Для вычисления близости слов используется косинусное расстояние между их векторами.\n",
    "\n",
    "\n",
    "\n",
    "С помощью дистрибутивных векторных моделей можно строить семантические пропорции (они же аналогии) и решать примеры:\n",
    "\n",
    "* *король: мужчина = королева: женщина* \n",
    " $\\Rightarrow$ \n",
    "* *король - мужчина + женщина = королева*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы\n",
    "1. Матрицы слишком разреженные (→ очень большие)\n",
    "2. Невозможно установить тип семантических отношений между словами: синонимы, антонимы и т.д. будут одинаково близки, потому что обычно употребляются в схожих контекстахю Поэтому близкие в векторном пространстве слова называют *семантическими ассоциатами*. Это значит, что они семантически связаны, но как именно — непонятно.\n",
    "\n",
    "\n",
    "## RusVectōrēs\n",
    "\n",
    "\n",
    "На сайте [RusVectōrēs](https://rusvectores.org/ru/) собраны предобученные на различных данных модели для русского языка, а также можно поискать наиболее близкие слова к заданному, посчитать семантическую близость нескольких слов и порешать примеры с помощью «калькулятором семантической близости».\n",
    "\n",
    "<img src=\"./img/rusvectores2.png\" width=\"500\">\n",
    "\n",
    "Для других языков также можно найти предобученные модели — например, модели [fastText](https://fasttext.cc/docs/en/english-vectors.html) и [GloVe](https://nlp.stanford.edu/projects/glove/) (о них чуть дальше)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "\n",
    "Использовать предобученную модель эмбеддингов или обучить свою можно с помощью библиотеки `gensim`. Вот [ее документация](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "### Как использовать готовую модель\n",
    "\n",
    "Модели word2vec бывают разных форматов:\n",
    "\n",
    "* .vec.gz — обычный файл\n",
    "* .bin.gz — бинарник\n",
    "\n",
    "Загружаются они с помощью одного и того же гласса `KeyedVectors`, меняется только параметр `binary` у функции `load_word2vec_format`. \n",
    "\n",
    "Если же эмбеддинги обучены **не** с помощью word2vec, то для загрузки нужно использовать функцию `load`. Т.е. для загрузки предобученных эмбеддингов *glove, fasttext, bpe* и любых других нужна именно она.\n",
    "\n",
    "Скачаем с RusVectōrēs модель для русского языка, обученную на НКРЯ образца 2015 г. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import logging\n",
    "import nltk.data \n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ruscorpora_mystem_cbow_300_2_2015.bin.gz',\n",
       " <http.client.HTTPMessage at 0x11aabb0b8>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
    "\n",
    "model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['день_S', 'ночь_S', 'человек_S', 'семантика_S', 'биткоин_S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частеречные тэги нужны, поскольку это специфика скачанной модели - она была натренирована на словах, аннотированных их частями речи (и лемматизированных). **NB!** В названиях моделей на `rusvectores` указано, какой тегсет они используют (mystem, upos и т.д.)\n",
    "\n",
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_S\n",
      "[-0.02580778  0.00970898  0.01941961 -0.02332282  0.02017624  0.07275085\n",
      " -0.01444375  0.03316632  0.01242602  0.02833412]\n",
      "неделя_S 0.7165195941925049\n",
      "месяц_S 0.631048858165741\n",
      "вечер_S 0.5828738808631897\n",
      "утро_S 0.5676206946372986\n",
      "час_S 0.5605547428131104\n",
      "минута_S 0.5297019481658936\n",
      "гекатомбеон_S 0.4897990822792053\n",
      "денек_S 0.48224714398384094\n",
      "полчаса_S 0.48217129707336426\n",
      "ночь_S 0.478074848651886\n",
      "\n",
      "\n",
      "ночь_S\n",
      "[-0.00688948  0.00408364  0.06975466 -0.00959525  0.0194835   0.04057068\n",
      " -0.00994112  0.06064967 -0.00522624  0.00520327]\n",
      "вечер_S 0.6946248412132263\n",
      "утро_S 0.57301926612854\n",
      "ноченька_S 0.5582467317581177\n",
      "рассвет_S 0.5553584098815918\n",
      "ночка_S 0.5351512432098389\n",
      "полдень_S 0.5334426760673523\n",
      "полночь_S 0.4786943793296814\n",
      "день_S 0.4780748784542084\n",
      "сумерки_S 0.4390218257904053\n",
      "фундерфун_S 0.4340824782848358\n",
      "\n",
      "\n",
      "человек_S\n",
      "[ 0.02013756 -0.02670703 -0.02039861 -0.05477146  0.00086402 -0.01636335\n",
      "  0.04240306 -0.00025525 -0.14045681  0.04785006]\n",
      "женщина_S 0.5979774594306946\n",
      "парень_S 0.4991787374019623\n",
      "мужчина_S 0.4767409861087799\n",
      "мужик_S 0.47383999824523926\n",
      "россиянин_S 0.4719043970108032\n",
      "народ_S 0.4654741883277893\n",
      "согражданин_S 0.45378515124320984\n",
      "горожанин_S 0.44368088245391846\n",
      "девушка_S 0.44314485788345337\n",
      "иностранец_S 0.43849867582321167\n",
      "\n",
      "\n",
      "семантика_S\n",
      "[-0.03066749  0.0053851   0.1110732   0.0152335   0.00440643  0.00384104\n",
      "  0.00096944 -0.03538784 -0.00079585  0.03220548]\n",
      "семантический_A 0.5334584712982178\n",
      "понятие_S 0.5030269622802734\n",
      "сочетаемость_S 0.4817051291465759\n",
      "актант_S 0.47596412897109985\n",
      "хронотоп_S 0.46330299973487854\n",
      "метафора_S 0.46158894896507263\n",
      "мышление_S 0.4610119163990021\n",
      "парадигма_S 0.4579666256904602\n",
      "лексема_S 0.45688074827194214\n",
      "смысловой_A 0.45430776476860046\n",
      "\n",
      "\n",
      "Увы, слова \"биткоин_S\" нет в модели!\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? \n",
    "    if word in model_ru:\n",
    "        print(word)\n",
    "        # смотрим на вектор слова (его размерность 300, смотрим на первые 10 чисел)\n",
    "        print(model_ru[word][:10])\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model_ru.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print('Увы, слова \"%s\" нет в модели!' % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2389560934322063\n"
     ]
    }
   ],
   "source": [
    "print(model_ru.similarity('человек_S', 'обезьяна_S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что получится, если вычесть из пиццы Италию и прибавить Сибирь?\n",
    "\n",
    "* positive — вектора, которые мы складываем\n",
    "* negative — вектора, которые вычитаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пельмень_S\n"
     ]
    }
   ],
   "source": [
    "print(model_ru.most_similar(positive=['пицца_S', 'сибирь_S'], negative=['италия_S'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как обучить свою модель\n",
    "\n",
    "В качестве обучающих данных возьмем размеченные и неразмеченные отзывы о фильмах (датасет взят с Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/w2v/train/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"./data/w2v/train/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"./data/w2v/train/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" \\\n",
    "      % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем из данных ссылки, html-разметку и небуквенные символы, а затем приводим все к нижнему регистру и токенизируем. На выходе получается массив из предложений, каждое из которых представляет собой массив слов. Здесь используется токенизатор из библиотеки `nltk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False ):\n",
    "    # убираем ссылки вне тегов\n",
    "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = stopwords.words(\"english\")\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = []  \n",
    "\n",
    "print(\"Parsing sentences from training set...\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set...\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем и сохраняем модель. \n",
    "\n",
    "\n",
    "Основные параметры:\n",
    "* данные должны быть итерируемым объектом \n",
    "* size — размер вектора, \n",
    "* window — размер окна наблюдения,\n",
    "* min_count — мин. частотность слова в корпусе,\n",
    "* sg — используемый алгоритм обучения (0 — CBOW, 1 — Skip-gram),\n",
    "* sample — порог для downsampling'a высокочастотных слов,\n",
    "* workers — количество потоков,\n",
    "* alpha — learning rate,\n",
    "* iter — количество итераций,\n",
    "* max_vocab_size — позволяет выставить ограничение по памяти при создании словаря (т.е. если ограничение привышается, то низкочастотные слова будут выбрасываться). Для сравнения: 10 млн слов = 1Гб RAM.\n",
    "\n",
    "**NB!** Обратите внимание, что тренировка модели не включает препроцессинг! Это значит, что избавляться от пунктуации, приводить слова к нижнему регистру, лемматизировать их, проставлять частеречные теги придется до тренировки модели (если, конечно, это необходимо для вашей задачи). Т.е. в каком виде слова будут в исходном тексте, в таком они будут и в модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:02:59,513 : INFO : collecting all words and their counts\n",
      "2019-09-05 13:02:59,515 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-09-05 13:02:59,591 : INFO : PROGRESS: at sentence #10000, processed 225800 words, keeping 17774 word types\n",
      "2019-09-05 13:02:59,657 : INFO : PROGRESS: at sentence #20000, processed 451863 words, keeping 24941 word types\n",
      "2019-09-05 13:02:59,709 : INFO : PROGRESS: at sentence #30000, processed 671259 words, keeping 30023 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:02:59,777 : INFO : PROGRESS: at sentence #40000, processed 897753 words, keeping 34338 word types\n",
      "2019-09-05 13:02:59,828 : INFO : PROGRESS: at sentence #50000, processed 1116873 words, keeping 37749 word types\n",
      "2019-09-05 13:02:59,882 : INFO : PROGRESS: at sentence #60000, processed 1338294 words, keeping 40708 word types\n",
      "2019-09-05 13:02:59,932 : INFO : PROGRESS: at sentence #70000, processed 1561470 words, keeping 43318 word types\n",
      "2019-09-05 13:02:59,989 : INFO : PROGRESS: at sentence #80000, processed 1780749 words, keeping 45696 word types\n",
      "2019-09-05 13:03:00,045 : INFO : PROGRESS: at sentence #90000, processed 2004848 words, keeping 48113 word types\n",
      "2019-09-05 13:03:00,098 : INFO : PROGRESS: at sentence #100000, processed 2226808 words, keeping 50184 word types\n",
      "2019-09-05 13:03:00,148 : INFO : PROGRESS: at sentence #110000, processed 2446405 words, keeping 52057 word types\n",
      "2019-09-05 13:03:00,200 : INFO : PROGRESS: at sentence #120000, processed 2668571 words, keeping 54091 word types\n",
      "2019-09-05 13:03:00,251 : INFO : PROGRESS: at sentence #130000, processed 2894095 words, keeping 55819 word types\n",
      "2019-09-05 13:03:00,311 : INFO : PROGRESS: at sentence #140000, processed 3106769 words, keeping 57315 word types\n",
      "2019-09-05 13:03:00,377 : INFO : PROGRESS: at sentence #150000, processed 3332383 words, keeping 59024 word types\n",
      "2019-09-05 13:03:00,441 : INFO : PROGRESS: at sentence #160000, processed 3555071 words, keeping 60586 word types\n",
      "2019-09-05 13:03:00,510 : INFO : PROGRESS: at sentence #170000, processed 3778402 words, keeping 62045 word types\n",
      "2019-09-05 13:03:00,580 : INFO : PROGRESS: at sentence #180000, processed 3998928 words, keeping 63458 word types\n",
      "2019-09-05 13:03:00,640 : INFO : PROGRESS: at sentence #190000, processed 4224124 words, keeping 64754 word types\n",
      "2019-09-05 13:03:00,692 : INFO : PROGRESS: at sentence #200000, processed 4448267 words, keeping 66045 word types\n",
      "2019-09-05 13:03:00,745 : INFO : PROGRESS: at sentence #210000, processed 4669611 words, keeping 67344 word types\n",
      "2019-09-05 13:03:00,810 : INFO : PROGRESS: at sentence #220000, processed 4894593 words, keeping 68652 word types\n",
      "2019-09-05 13:03:00,860 : INFO : PROGRESS: at sentence #230000, processed 5117163 words, keeping 69912 word types\n",
      "2019-09-05 13:03:00,912 : INFO : PROGRESS: at sentence #240000, processed 5344645 words, keeping 71119 word types\n",
      "2019-09-05 13:03:00,977 : INFO : PROGRESS: at sentence #250000, processed 5558760 words, keeping 72303 word types\n",
      "2019-09-05 13:03:01,034 : INFO : PROGRESS: at sentence #260000, processed 5778737 words, keeping 73429 word types\n",
      "2019-09-05 13:03:01,086 : INFO : PROGRESS: at sentence #270000, processed 6000010 words, keeping 74717 word types\n",
      "2019-09-05 13:03:01,145 : INFO : PROGRESS: at sentence #280000, processed 6225883 words, keeping 76319 word types\n",
      "2019-09-05 13:03:01,202 : INFO : PROGRESS: at sentence #290000, processed 6449035 words, keeping 77787 word types\n",
      "2019-09-05 13:03:01,258 : INFO : PROGRESS: at sentence #300000, processed 6673613 words, keeping 79115 word types\n",
      "2019-09-05 13:03:01,320 : INFO : PROGRESS: at sentence #310000, processed 6898916 words, keeping 80424 word types\n",
      "2019-09-05 13:03:01,387 : INFO : PROGRESS: at sentence #320000, processed 7123789 words, keeping 81752 word types\n",
      "2019-09-05 13:03:01,450 : INFO : PROGRESS: at sentence #330000, processed 7345526 words, keeping 82974 word types\n",
      "2019-09-05 13:03:01,503 : INFO : PROGRESS: at sentence #340000, processed 7575023 words, keeping 84219 word types\n",
      "2019-09-05 13:03:01,556 : INFO : PROGRESS: at sentence #350000, processed 7798275 words, keeping 85364 word types\n",
      "2019-09-05 13:03:01,615 : INFO : PROGRESS: at sentence #360000, processed 8018874 words, keeping 86531 word types\n",
      "2019-09-05 13:03:01,665 : INFO : PROGRESS: at sentence #370000, processed 8246066 words, keeping 87643 word types\n",
      "2019-09-05 13:03:01,731 : INFO : PROGRESS: at sentence #380000, processed 8471186 words, keeping 88811 word types\n",
      "2019-09-05 13:03:01,782 : INFO : PROGRESS: at sentence #390000, processed 8700909 words, keeping 89838 word types\n",
      "2019-09-05 13:03:01,838 : INFO : PROGRESS: at sentence #400000, processed 8923833 words, keeping 90847 word types\n",
      "2019-09-05 13:03:01,889 : INFO : PROGRESS: at sentence #410000, processed 9145166 words, keeping 91810 word types\n",
      "2019-09-05 13:03:01,939 : INFO : PROGRESS: at sentence #420000, processed 9366231 words, keeping 92840 word types\n",
      "2019-09-05 13:03:01,994 : INFO : PROGRESS: at sentence #430000, processed 9593763 words, keeping 93858 word types\n",
      "2019-09-05 13:03:02,048 : INFO : PROGRESS: at sentence #440000, processed 9820504 words, keeping 94832 word types\n",
      "2019-09-05 13:03:02,100 : INFO : PROGRESS: at sentence #450000, processed 10044254 words, keeping 95960 word types\n",
      "2019-09-05 13:03:02,151 : INFO : PROGRESS: at sentence #460000, processed 10276995 words, keeping 97012 word types\n",
      "2019-09-05 13:03:02,207 : INFO : PROGRESS: at sentence #470000, processed 10504886 words, keeping 97854 word types\n",
      "2019-09-05 13:03:02,258 : INFO : PROGRESS: at sentence #480000, processed 10725265 words, keeping 98783 word types\n",
      "2019-09-05 13:03:02,313 : INFO : PROGRESS: at sentence #490000, processed 10951923 words, keeping 99787 word types\n",
      "2019-09-05 13:03:02,363 : INFO : PROGRESS: at sentence #500000, processed 11173555 words, keeping 100679 word types\n",
      "2019-09-05 13:03:02,418 : INFO : PROGRESS: at sentence #510000, processed 11398824 words, keeping 101612 word types\n",
      "2019-09-05 13:03:02,469 : INFO : PROGRESS: at sentence #520000, processed 11622159 words, keeping 102511 word types\n",
      "2019-09-05 13:03:02,525 : INFO : PROGRESS: at sentence #530000, processed 11846542 words, keeping 103312 word types\n",
      "2019-09-05 13:03:02,576 : INFO : PROGRESS: at sentence #540000, processed 12071124 words, keeping 104171 word types\n",
      "2019-09-05 13:03:02,628 : INFO : PROGRESS: at sentence #550000, processed 12296653 words, keeping 105037 word types\n",
      "2019-09-05 13:03:02,685 : INFO : PROGRESS: at sentence #560000, processed 12517927 words, keeping 105901 word types\n",
      "2019-09-05 13:03:02,739 : INFO : PROGRESS: at sentence #570000, processed 12746971 words, keeping 106689 word types\n",
      "2019-09-05 13:03:02,790 : INFO : PROGRESS: at sentence #580000, processed 12968451 words, keeping 107565 word types\n",
      "2019-09-05 13:03:02,844 : INFO : PROGRESS: at sentence #590000, processed 13193962 words, keeping 108401 word types\n",
      "2019-09-05 13:03:02,896 : INFO : PROGRESS: at sentence #600000, processed 13416148 words, keeping 109117 word types\n",
      "2019-09-05 13:03:02,953 : INFO : PROGRESS: at sentence #610000, processed 13637150 words, keeping 109989 word types\n",
      "2019-09-05 13:03:03,003 : INFO : PROGRESS: at sentence #620000, processed 13863432 words, keeping 110731 word types\n",
      "2019-09-05 13:03:03,057 : INFO : PROGRESS: at sentence #630000, processed 14087718 words, keeping 111505 word types\n",
      "2019-09-05 13:03:03,111 : INFO : PROGRESS: at sentence #640000, processed 14308493 words, keeping 112308 word types\n",
      "2019-09-05 13:03:03,164 : INFO : PROGRESS: at sentence #650000, processed 14534231 words, keeping 113088 word types\n",
      "2019-09-05 13:03:03,217 : INFO : PROGRESS: at sentence #660000, processed 14757002 words, keeping 113837 word types\n",
      "2019-09-05 13:03:03,285 : INFO : PROGRESS: at sentence #670000, processed 14980380 words, keeping 114533 word types\n",
      "2019-09-05 13:03:03,342 : INFO : PROGRESS: at sentence #680000, processed 15205199 words, keeping 115244 word types\n",
      "2019-09-05 13:03:03,392 : INFO : PROGRESS: at sentence #690000, processed 15427386 words, keeping 116021 word types\n",
      "2019-09-05 13:03:03,451 : INFO : PROGRESS: at sentence #700000, processed 15656076 words, keeping 116833 word types\n",
      "2019-09-05 13:03:03,513 : INFO : PROGRESS: at sentence #710000, processed 15879053 words, keeping 117485 word types\n",
      "2019-09-05 13:03:03,580 : INFO : PROGRESS: at sentence #720000, processed 16104322 words, keeping 118108 word types\n",
      "2019-09-05 13:03:03,634 : INFO : PROGRESS: at sentence #730000, processed 16330689 words, keeping 118838 word types\n",
      "2019-09-05 13:03:03,687 : INFO : PROGRESS: at sentence #740000, processed 16551699 words, keeping 119547 word types\n",
      "2019-09-05 13:03:03,740 : INFO : PROGRESS: at sentence #750000, processed 16770015 words, keeping 120171 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:03:03,790 : INFO : PROGRESS: at sentence #760000, processed 16989388 words, keeping 120805 word types\n",
      "2019-09-05 13:03:03,844 : INFO : PROGRESS: at sentence #770000, processed 17216519 words, keeping 121578 word types\n",
      "2019-09-05 13:03:03,895 : INFO : PROGRESS: at sentence #780000, processed 17446646 words, keeping 122275 word types\n",
      "2019-09-05 13:03:03,967 : INFO : PROGRESS: at sentence #790000, processed 17673722 words, keeping 122939 word types\n",
      "2019-09-05 13:03:04,045 : INFO : collected 123376 word types from a corpus of 17796809 raw words and 795538 sentences\n",
      "2019-09-05 13:03:04,046 : INFO : Loading a fresh vocabulary\n",
      "2019-09-05 13:03:04,217 : INFO : min_count=10 retains 34112 unique words (27% of original 123376, drops 89264)\n",
      "2019-09-05 13:03:04,218 : INFO : min_count=10 leaves 17588810 word corpus (98% of original 17796809, drops 207999)\n",
      "2019-09-05 13:03:04,371 : INFO : deleting the raw counts dictionary of 123376 items\n",
      "2019-09-05 13:03:04,377 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-09-05 13:03:04,378 : INFO : downsampling leaves estimated 13139433 word corpus (74.7% of prior 17588810)\n",
      "2019-09-05 13:03:04,535 : INFO : estimated required memory for 34112 words and 300 dimensions: 98924800 bytes\n",
      "2019-09-05 13:03:04,536 : INFO : resetting layer weights\n",
      "2019-09-05 13:03:05,003 : INFO : training model with 4 workers on 34112 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-09-05 13:03:06,014 : INFO : EPOCH 1 - PROGRESS: at 5.65% examples, 741383 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:07,035 : INFO : EPOCH 1 - PROGRESS: at 11.26% examples, 727755 words/s, in_qsize 6, out_qsize 2\n",
      "2019-09-05 13:03:08,041 : INFO : EPOCH 1 - PROGRESS: at 17.46% examples, 751515 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:09,052 : INFO : EPOCH 1 - PROGRESS: at 23.33% examples, 753341 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:10,079 : INFO : EPOCH 1 - PROGRESS: at 27.92% examples, 719887 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:11,090 : INFO : EPOCH 1 - PROGRESS: at 34.22% examples, 734763 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:12,093 : INFO : EPOCH 1 - PROGRESS: at 40.31% examples, 744054 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:13,098 : INFO : EPOCH 1 - PROGRESS: at 46.83% examples, 758161 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:14,101 : INFO : EPOCH 1 - PROGRESS: at 53.16% examples, 766104 words/s, in_qsize 8, out_qsize 0\n",
      "2019-09-05 13:03:15,114 : INFO : EPOCH 1 - PROGRESS: at 59.17% examples, 768713 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:16,126 : INFO : EPOCH 1 - PROGRESS: at 64.50% examples, 761740 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:17,138 : INFO : EPOCH 1 - PROGRESS: at 70.28% examples, 760740 words/s, in_qsize 8, out_qsize 1\n",
      "2019-09-05 13:03:18,140 : INFO : EPOCH 1 - PROGRESS: at 75.25% examples, 752724 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:19,150 : INFO : EPOCH 1 - PROGRESS: at 80.31% examples, 745826 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:20,153 : INFO : EPOCH 1 - PROGRESS: at 85.23% examples, 739171 words/s, in_qsize 8, out_qsize 0\n",
      "2019-09-05 13:03:21,155 : INFO : EPOCH 1 - PROGRESS: at 89.58% examples, 728891 words/s, in_qsize 6, out_qsize 1\n",
      "2019-09-05 13:03:22,165 : INFO : EPOCH 1 - PROGRESS: at 95.86% examples, 733698 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:22,829 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:03:22,830 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:03:22,847 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:03:22,854 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:03:22,854 : INFO : EPOCH - 1 : training on 17796809 raw words (13138316 effective words) took 17.8s, 736274 effective words/s\n",
      "2019-09-05 13:03:23,876 : INFO : EPOCH 2 - PROGRESS: at 6.56% examples, 858476 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:24,882 : INFO : EPOCH 2 - PROGRESS: at 12.87% examples, 836236 words/s, in_qsize 8, out_qsize 0\n",
      "2019-09-05 13:03:25,892 : INFO : EPOCH 2 - PROGRESS: at 18.92% examples, 817540 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:26,908 : INFO : EPOCH 2 - PROGRESS: at 23.70% examples, 767125 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:27,922 : INFO : EPOCH 2 - PROGRESS: at 29.05% examples, 751802 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:28,929 : INFO : EPOCH 2 - PROGRESS: at 35.11% examples, 757012 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:29,932 : INFO : EPOCH 2 - PROGRESS: at 41.27% examples, 764275 words/s, in_qsize 8, out_qsize 0\n",
      "2019-09-05 13:03:30,946 : INFO : EPOCH 2 - PROGRESS: at 47.90% examples, 776826 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:31,955 : INFO : EPOCH 2 - PROGRESS: at 54.60% examples, 787894 words/s, in_qsize 6, out_qsize 1\n",
      "2019-09-05 13:03:32,963 : INFO : EPOCH 2 - PROGRESS: at 61.13% examples, 795411 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:33,964 : INFO : EPOCH 2 - PROGRESS: at 67.70% examples, 801379 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:34,967 : INFO : EPOCH 2 - PROGRESS: at 73.85% examples, 802008 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:35,980 : INFO : EPOCH 2 - PROGRESS: at 80.53% examples, 806926 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:36,980 : INFO : EPOCH 2 - PROGRESS: at 86.25% examples, 802996 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:37,996 : INFO : EPOCH 2 - PROGRESS: at 92.50% examples, 803685 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:38,996 : INFO : EPOCH 2 - PROGRESS: at 98.77% examples, 804995 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:39,203 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:03:39,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:03:39,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:03:39,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:03:39,237 : INFO : EPOCH - 2 : training on 17796809 raw words (13142303 effective words) took 16.4s, 803008 effective words/s\n",
      "2019-09-05 13:03:40,263 : INFO : EPOCH 3 - PROGRESS: at 4.38% examples, 569184 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:41,279 : INFO : EPOCH 3 - PROGRESS: at 10.92% examples, 705288 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:42,297 : INFO : EPOCH 3 - PROGRESS: at 17.69% examples, 757831 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:43,300 : INFO : EPOCH 3 - PROGRESS: at 23.38% examples, 753896 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:44,325 : INFO : EPOCH 3 - PROGRESS: at 28.37% examples, 730980 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:45,334 : INFO : EPOCH 3 - PROGRESS: at 33.16% examples, 711360 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:46,365 : INFO : EPOCH 3 - PROGRESS: at 38.30% examples, 703560 words/s, in_qsize 5, out_qsize 2\n",
      "2019-09-05 13:03:47,390 : INFO : EPOCH 3 - PROGRESS: at 42.64% examples, 685521 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:48,401 : INFO : EPOCH 3 - PROGRESS: at 47.95% examples, 686264 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:49,408 : INFO : EPOCH 3 - PROGRESS: at 53.05% examples, 684298 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:50,408 : INFO : EPOCH 3 - PROGRESS: at 58.52% examples, 688330 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:51,410 : INFO : EPOCH 3 - PROGRESS: at 64.90% examples, 700786 words/s, in_qsize 8, out_qsize 0\n",
      "2019-09-05 13:03:52,410 : INFO : EPOCH 3 - PROGRESS: at 71.27% examples, 711417 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:53,410 : INFO : EPOCH 3 - PROGRESS: at 77.84% examples, 722086 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:54,415 : INFO : EPOCH 3 - PROGRESS: at 84.34% examples, 730566 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:55,424 : INFO : EPOCH 3 - PROGRESS: at 90.59% examples, 736047 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:56,424 : INFO : EPOCH 3 - PROGRESS: at 97.07% examples, 742516 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:56,867 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:03:56,873 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:03:56,891 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:03:56,892 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:03:56,893 : INFO : EPOCH - 3 : training on 17796809 raw words (13139905 effective words) took 17.6s, 744866 effective words/s\n",
      "2019-09-05 13:03:57,914 : INFO : EPOCH 4 - PROGRESS: at 6.56% examples, 854694 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:58,919 : INFO : EPOCH 4 - PROGRESS: at 13.25% examples, 860270 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:03:59,920 : INFO : EPOCH 4 - PROGRESS: at 19.88% examples, 860745 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:00,930 : INFO : EPOCH 4 - PROGRESS: at 26.41% examples, 857125 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:01,945 : INFO : EPOCH 4 - PROGRESS: at 33.04% examples, 855707 words/s, in_qsize 6, out_qsize 1\n",
      "2019-09-05 13:04:02,949 : INFO : EPOCH 4 - PROGRESS: at 39.54% examples, 854987 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:03,953 : INFO : EPOCH 4 - PROGRESS: at 46.22% examples, 858633 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:04,958 : INFO : EPOCH 4 - PROGRESS: at 52.89% examples, 860362 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:05,964 : INFO : EPOCH 4 - PROGRESS: at 59.40% examples, 860828 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:06,969 : INFO : EPOCH 4 - PROGRESS: at 65.95% examples, 860507 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:07,987 : INFO : EPOCH 4 - PROGRESS: at 72.55% examples, 860008 words/s, in_qsize 8, out_qsize 1\n",
      "2019-09-05 13:04:08,992 : INFO : EPOCH 4 - PROGRESS: at 79.12% examples, 859838 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:10,010 : INFO : EPOCH 4 - PROGRESS: at 85.00% examples, 852013 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:11,022 : INFO : EPOCH 4 - PROGRESS: at 91.04% examples, 847298 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:12,026 : INFO : EPOCH 4 - PROGRESS: at 96.96% examples, 842230 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:12,517 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:04:12,536 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:04:12,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:04:12,539 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:04:12,540 : INFO : EPOCH - 4 : training on 17796809 raw words (13140612 effective words) took 15.6s, 840451 effective words/s\n",
      "2019-09-05 13:04:13,578 : INFO : EPOCH 5 - PROGRESS: at 5.53% examples, 713042 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:14,588 : INFO : EPOCH 5 - PROGRESS: at 10.37% examples, 666853 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:15,594 : INFO : EPOCH 5 - PROGRESS: at 15.35% examples, 659524 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:16,595 : INFO : EPOCH 5 - PROGRESS: at 21.12% examples, 682158 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:17,600 : INFO : EPOCH 5 - PROGRESS: at 27.64% examples, 715712 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:18,607 : INFO : EPOCH 5 - PROGRESS: at 34.44% examples, 742754 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:19,618 : INFO : EPOCH 5 - PROGRESS: at 40.08% examples, 741810 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:20,619 : INFO : EPOCH 5 - PROGRESS: at 45.56% examples, 739272 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:21,631 : INFO : EPOCH 5 - PROGRESS: at 51.99% examples, 750282 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:22,644 : INFO : EPOCH 5 - PROGRESS: at 58.58% examples, 761861 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:23,648 : INFO : EPOCH 5 - PROGRESS: at 65.24% examples, 772056 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:24,653 : INFO : EPOCH 5 - PROGRESS: at 71.93% examples, 781030 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:25,669 : INFO : EPOCH 5 - PROGRESS: at 78.68% examples, 787980 words/s, in_qsize 6, out_qsize 1\n",
      "2019-09-05 13:04:26,671 : INFO : EPOCH 5 - PROGRESS: at 85.40% examples, 794639 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:27,684 : INFO : EPOCH 5 - PROGRESS: at 92.04% examples, 799434 words/s, in_qsize 7, out_qsize 0\n",
      "2019-09-05 13:04:28,698 : INFO : EPOCH 5 - PROGRESS: at 98.29% examples, 799893 words/s, in_qsize 8, out_qsize 1\n",
      "2019-09-05 13:04:28,956 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:04:28,957 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:04:28,975 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:04:28,979 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:04:28,980 : INFO : EPOCH - 5 : training on 17796809 raw words (13140562 effective words) took 16.4s, 800060 effective words/s\n",
      "2019-09-05 13:04:28,980 : INFO : training on a 88984045 raw words (65701698 effective words) took 84.0s, 782404 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 38s, sys: 1.85 s, total: 4min 40s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "%time model_en = word2vec.Word2Vec(sentences, workers=4, size=300, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, сколько в модели слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34112\n"
     ]
    }
   ],
   "source": [
    "print(len(model_en.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем оценить модель вручную, порешав примеры. Несколько дано ниже, попробуйте придумать свои."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:04:29,001 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('actress', 0.7271527051925659)]\n",
      "[('men', 0.6663469672203064)]\n",
      "[('uk', 0.7185591459274292), ('germany', 0.7098673582077026), ('europe', 0.7065819501876831)]\n",
      "novel\n"
     ]
    }
   ],
   "source": [
    "print(model_en.wv.most_similar(positive=[\"woman\", \"actor\"], negative=[\"man\"], topn=1))\n",
    "print(model_en.wv.most_similar(positive=[\"dogs\", \"man\"], negative=[\"dog\"], topn=1))\n",
    "\n",
    "print(model_en.wv.most_similar(\"usa\", topn=3))\n",
    "\n",
    "print(model_en.wv.doesnt_match(\"comedy thriller western novel\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как дообучить существующую модель\n",
    "\n",
    "При тренировке модели \"с нуля\" веса инициализируются случайно, однако можно использовать для инициализации векторов веса из предобученной модели, таким образом как бы дообучая ее.\n",
    "\n",
    "Сначала посмотрим близость какой-нибудь пары слов в имеющейс модели, чтобы потом сравнить результат с дообученной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36310929940799597"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.wv.similarity('lion', 'unicorn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве дополнительных данных для обучения возьмем английский текст «Алисы в Зазеркалье»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['through', 'the', 'looking-glass', 'by', 'lewis', 'carroll', 'chapter', 'i', 'looking-glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', '', 'it', 'was', 'the', 'black', 'kitten’s', 'fault', 'entirely'], ['for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', 'and', 'bearing', 'it', 'pretty', 'well', 'considering', 'so', 'you', 'see', 'that', 'it', 'couldn’t', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/w2v/train/alice.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub('\\n', ' ', text)\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~„“«»†*—/\\-‘’'\n",
    "clean_sents = []\n",
    "\n",
    "for sent in sents:\n",
    "    s = [w.lower().strip(punct) for w in sent.split()]\n",
    "    clean_sents.append(s)\n",
    "    \n",
    "print(clean_sents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы дообучить модель, надо сначала ее сохранить, а потом загрузить. Все параметры тренировки (размер вектора, мин. частота слова и т.п.) будут взяты из загруженной модели, т.е. задать их заново нельзя.\n",
    "\n",
    "**NB!** Дообучить можно только полную модель, а `KeyedVectors` — нельзя. Поэтому сохранять модель нужно в соотвествующем формате. Подробнее о разнице [вот тут](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:07:55,269 : INFO : saving Word2Vec object under ./data/w2v/movie_reviews.model, separately None\n",
      "2019-09-05 13:07:55,271 : INFO : not storing attribute vectors_norm\n",
      "2019-09-05 13:07:55,274 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:07:56,359 : INFO : saved ./data/w2v/movie_reviews.model\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./data/w2v/movie_reviews.model\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_en.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:08:45,036 : INFO : loading Word2Vec object from ./data/w2v/movie_reviews.model\n",
      "2019-09-05 13:08:46,211 : INFO : loading wv recursively from ./data/w2v/movie_reviews.model.wv.* with mmap=None\n",
      "2019-09-05 13:08:46,212 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-09-05 13:08:46,218 : INFO : loading vocabulary recursively from ./data/w2v/movie_reviews.model.vocabulary.* with mmap=None\n",
      "2019-09-05 13:08:46,219 : INFO : loading trainables recursively from ./data/w2v/movie_reviews.model.trainables.* with mmap=None\n",
      "2019-09-05 13:08:46,244 : INFO : setting ignored attribute cum_table to None\n",
      "2019-09-05 13:08:46,247 : INFO : loaded ./data/w2v/movie_reviews.model\n",
      "2019-09-05 13:08:46,338 : INFO : collecting all words and their counts\n",
      "2019-09-05 13:08:46,340 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-09-05 13:08:46,373 : INFO : collected 2956 word types from a corpus of 30045 raw words and 1222 sentences\n",
      "2019-09-05 13:08:46,374 : INFO : Updating model with new vocabulary\n",
      "2019-09-05 13:08:46,388 : INFO : New added 410 unique words (12% of original 3366) and increased the count of 410 pre-existing words (12% of original 3366)\n",
      "2019-09-05 13:08:46,405 : INFO : deleting the raw counts dictionary of 2956 items\n",
      "2019-09-05 13:08:46,412 : INFO : sample=0.001 downsamples 158 most-common words\n",
      "2019-09-05 13:08:46,422 : INFO : downsampling leaves estimated 29258 word corpus (120.6% of prior 24256)\n",
      "2019-09-05 13:08:46,594 : INFO : estimated required memory for 820 words and 300 dimensions: 2378000 bytes\n",
      "2019-09-05 13:08:46,597 : INFO : updating layer weights\n",
      "2019-09-05 13:08:46,641 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-09-05 13:08:46,642 : INFO : training model with 4 workers on 34152 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-09-05 13:08:46,669 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:08:46,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:08:46,685 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:08:46,692 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:08:46,695 : INFO : EPOCH - 1 : training on 30045 raw words (19577 effective words) took 0.0s, 803794 effective words/s\n",
      "2019-09-05 13:08:46,735 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:08:46,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:08:46,747 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:08:46,751 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:08:46,751 : INFO : EPOCH - 2 : training on 30045 raw words (19484 effective words) took 0.0s, 457645 effective words/s\n",
      "2019-09-05 13:08:46,797 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:08:46,809 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:08:46,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:08:46,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:08:46,813 : INFO : EPOCH - 3 : training on 30045 raw words (19507 effective words) took 0.0s, 440787 effective words/s\n",
      "2019-09-05 13:08:46,849 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:08:46,850 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:08:46,862 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:08:46,863 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:08:46,863 : INFO : EPOCH - 4 : training on 30045 raw words (19476 effective words) took 0.0s, 489207 effective words/s\n",
      "2019-09-05 13:08:46,900 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-05 13:08:46,905 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-05 13:08:46,911 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-05 13:08:46,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-05 13:08:46,912 : INFO : EPOCH - 5 : training on 30045 raw words (19543 effective words) took 0.0s, 579733 effective words/s\n",
      "2019-09-05 13:08:46,913 : INFO : training on a 150225 raw words (97587 effective words) took 0.3s, 364224 effective words/s\n",
      "2019-09-05 13:08:46,914 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97587, 150225)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(model_path)\n",
    "\n",
    "model.build_vocab(clean_sents, update=True)\n",
    "model.train(clean_sents, total_examples=model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лев и единорог стали гораздо ближе друг к другу!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48794380396529935"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('lion', 'unicorn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать. Здесь используется L2-нормализация: вектора нормализуются так, что если сложить квадраты всех элементов вектора, в сумме получится 1. \n",
    "\n",
    "Кроме того, сохраним не полные вектора, а `KeyedVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:08:59,233 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-09-05 13:08:59,549 : INFO : storing 34112x300 projection weights into ./data/w2v/movies_alice.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)\n",
    "model_path = \"./data/w2v/movies_alice.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_en.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка\n",
    "\n",
    "Это, конечно, хорошо, но как понять, какая модель лучше? Или вот, например, я сделал свою модель, а как понять, насколько она хорошая?\n",
    "\n",
    "Для этого существуют специальные датасеты для оценки качества дистрибутивных моделей. Основных два: один измеряет точность решения задач на аналогии (про Россию и пельмени), а второй используется для оценки коэффициента семантической близости. \n",
    "\n",
    "### Word Similarity\n",
    "\n",
    "Этот метод заключается в том, чтобы оценить, насколько представления о семантической близости слов в модели соотносятся с \"представлениями\" людей.\n",
    "\n",
    "| слово 1    | слово 2    | близость | \n",
    "|------------|------------|----------|\n",
    "| кошка      | собака     | 0.7      |  \n",
    "| чашка      | кружка     | 0.9      |       \n",
    "\n",
    "Для каждой пары слов из заранее заданного датасета мы можем посчитать косинусное расстояние, и получить список таких значений близости. При этом у нас уже есть список значений близостей, сделанный людьми. Мы можем сравнить эти два списка и понять, насколько они похожи (например, посчитав корреляцию). Эта мера схожести должна говорить о том, насколько модель хорошо моделирует расстояния о слова.\n",
    "\n",
    "### Аналогии\n",
    "\n",
    "Другая популярная задача для \"внутренней\" оценки называется задачей поиска аналогий. Как мы уже разбирали выше, с помощью простых арифметических операций мы можем модифицировать значение слова. Если заранее собрать набор слов-модификаторов, а также слов, которые мы хотим получить в результаты модификации, то на основе подсчёта количества \"попаданий\" в желаемое слово мы можем оценить, насколько хорошо работает модель.\n",
    "\n",
    "В качестве слов-модификатор мы можем использовать семантические аналогии. Скажем, если у нас есть некоторое отношение \"страна-столица\", то для оценки модели мы можем использовать пары наподобие \"Россия-Москва\", \"Норвегия-Осло\", и т.д. Датасет будет выглядеть следующм образом:\n",
    "\n",
    "| слово 1    | слово 2    | отношение     | \n",
    "|------------|------------|---------------|\n",
    "| Россия     | Москва     | страна-столица|  \n",
    "| Норвегия   | Осло       | страна-столица|\n",
    "\n",
    "Рассматривая случайные две пары из этого набора, мы хотим, имея триплет (Россия, Москва, Норвегия) хотим получить слово \"Осло\", т.е. найти такое слово, которое будет находиться в том же отношении со словом \"Норвегия\", как \"Россия\" находится с Москвой. \n",
    "\n",
    "Датасеты для русского языка можно скачать на странице с моделями на RusVectores. Посчитаем качество нашей модели НКРЯ на датасете про аналогии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:09:06,330 : INFO : capital-common-countries: 19.0% (58/306)\n",
      "2019-09-05 13:09:08,038 : INFO : capital-world: 10.1% (52/515)\n",
      "2019-09-05 13:09:08,501 : INFO : currency: 4.6% (6/130)\n",
      "2019-09-05 13:09:09,529 : INFO : family: 71.2% (218/306)\n",
      "2019-09-05 13:09:12,117 : INFO : gram1-Aective-to-adverb: 18.7% (152/812)\n",
      "2019-09-05 13:09:13,366 : INFO : gram2-opposite: 32.1% (122/380)\n",
      "2019-09-05 13:09:16,239 : INFO : gram6-nationality-Aective: 32.3% (293/907)\n",
      "2019-09-05 13:09:16,240 : INFO : total: 26.8% (901/3356)\n"
     ]
    }
   ],
   "source": [
    "res = model_ru.accuracy('./data/w2v/evaluation/ru_analogy_tagged.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('МАЛЬЧИК_S', 'ДЕВОЧКА_S', 'ДЕД_S', 'БАБКА_S'), ('МАЛЬЧИК_S', 'ДЕВОЧКА_S', 'КОРОЛЬ_S', 'КОРОЛЕВА_S'), ('МАЛЬЧИК_S', 'ДЕВОЧКА_S', 'ПРИНЦ_S', 'ПРИНЦЕССА_S'), ('МАЛЬЧИК_S', 'ДЕВОЧКА_S', 'ОТЧИМ_S', 'МАЧЕХА_S'), ('МАЛЬЧИК_S', 'ДЕВОЧКА_S', 'ПАСЫНОК_S', 'ПАДЧЕРИЦА_S'), ('БРАТ_S', 'СЕСТРА_S', 'ДЕД_S', 'БАБКА_S'), ('БРАТ_S', 'СЕСТРА_S', 'ОТЧИМ_S', 'МАЧЕХА_S'), ('БРАТ_S', 'СЕСТРА_S', 'ПАСЫНОК_S', 'ПАДЧЕРИЦА_S'), ('ПАПА_S', 'МАМА_S', 'ДЕД_S', 'БАБКА_S'), ('ПАПА_S', 'МАМА_S', 'ОТЧИМ_S', 'МАЧЕХА_S')]\n"
     ]
    }
   ],
   "source": [
    "print(res[4]['incorrect'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vecto\n",
    "\n",
    "Также для \"внутренней\" (intrinsic) оценки, т.е. оценки в отрыве от конкретной задачи, можно использовать фреймворк [vecto](https://vecto.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecto.benchmarks.analogy import Analogy\n",
    "from vecto.benchmarks.similarity import Similarity\n",
    "import vecto.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vecto использует свои собственные структуры для работы с векторными моделями, поэтому нужно загрузить модель в память ещё раз. Для простоты загрузим только одну из рассмотренных моделей, Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-05 13:10:06,075 : INFO : Detected w2v original binary format\n"
     ]
    }
   ],
   "source": [
    "embeddings = vecto.embeddings.load_from_dir(\"./data/w2v/\")\n",
    "embeddings.cache_normalized_copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Vecto можно также как и в Gensim получать вектора для слов или искать наиболее похожие слова из словаря модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['movie', 1.0],\n",
       " ['film', 0.92556167],\n",
       " ['flick', 0.836619],\n",
       " ['movies', 0.7876787],\n",
       " ['it', 0.78067106],\n",
       " ['sequel', 0.76701987],\n",
       " ['picture', 0.75892735],\n",
       " ['documentary', 0.754131],\n",
       " ['storyline', 0.73823464],\n",
       " ['show', 0.73598826]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.get_most_similar_words('movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment_setup': {'cnt_found_pairs_total': 2,\n",
       "   'cnt_pairs_total': 2,\n",
       "   'embeddings': {'_class': 'vecto.embeddings.legacy_w2v.ModelW2V',\n",
       "    'normalized': True},\n",
       "   'category': 'default',\n",
       "   'dataset': 'ws',\n",
       "   'method': 'cosine_distance',\n",
       "   'language': 'en',\n",
       "   'description': 'TEST FILE',\n",
       "   'version': '-',\n",
       "   'measurement': 'spearman',\n",
       "   'task': 'similarity',\n",
       "   'timestamp': '2019-04-12T17:46:45.086542',\n",
       "   'default_measurement': 'spearman'},\n",
       "  'result': {'spearman': -1},\n",
       "  'details': []}]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = Similarity()\n",
    "similarity.get_result(embeddings, path_dataset='./data/w2v/evaluation/similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-12 17:45:29,938 : INFO : processing ./data/w2v/evaluation/analogy/category1/subcategory_a.txt\n",
      "N/A% (0 of 2) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--2019-04-12 17:45:30,003 : INFO : processing ./data/w2v/evaluation/analogy/category2/subcategory_b.txt\n",
      "N/A% (0 of 4) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'details': [{'question verbose': \"What is to apple as ['quick'] is to fast\",\n",
       "    'b': 'apple',\n",
       "    'expected answer': ['banana'],\n",
       "    'predictions': [{'score': 0.728181004524231,\n",
       "      'answer': 'lebowski',\n",
       "      'hit': False},\n",
       "     {'score': 0.726725697517395, 'answer': 'midget', 'hit': False},\n",
       "     {'score': 0.7247903347015381, 'answer': 'booty', 'hit': False},\n",
       "     {'score': 0.7216457724571228, 'answer': 'coat', 'hit': False},\n",
       "     {'score': 0.7199649214744568, 'answer': 'sauce', 'hit': False},\n",
       "     {'score': 0.7191541194915771, 'answer': 'horn', 'hit': False}],\n",
       "    'set_exclude': ['fast', 'apple', 'quick'],\n",
       "    'rank': 816,\n",
       "    'landing_b': True,\n",
       "    'landing_b_prime': False,\n",
       "    'landing_a': False,\n",
       "    'landing_a_prime': False,\n",
       "    'similarity predicted to b_prime cosine': 0.6400811225175858,\n",
       "    'similarity a to a_prime cosine': 0.7612074613571167,\n",
       "    'similarity a_prime to b_prime cosine': 0.5334567055106163,\n",
       "    'similarity b to b_prime cosine': 0.670642763376236,\n",
       "    'similarity a to b_prime cosine': 0.5013529043644667,\n",
       "    'distance a to a_prime euclidean': 0.9773280620574951,\n",
       "    'distance a_prime to b_prime euclidean': 1.366079568862915,\n",
       "    'distance b to b_prime euclidean': 1.147793173789978,\n",
       "    'distance a to b_prime euclidean': 1.4122990369796753,\n",
       "    'crowdedness of b_prime': [0.8023819923400879,\n",
       "     0.8016062378883362,\n",
       "     0.7981831431388855,\n",
       "     0.7968431711196899,\n",
       "     0.7955393195152283,\n",
       "     0.7927921414375305,\n",
       "     0.7924220561981201,\n",
       "     0.7920680046081543,\n",
       "     0.7914565801620483,\n",
       "     0.7905268669128418],\n",
       "    'b in neighbourhood of b_prime': 8154,\n",
       "    'b_prime in neighbourhood of b': 486},\n",
       "   {'question verbose': \"What is to fast as ['banana'] is to apple\",\n",
       "    'b': 'fast',\n",
       "    'expected answer': ['quick'],\n",
       "    'predictions': [{'score': 0.7522187829017639,\n",
       "      'answer': 'leisurely',\n",
       "      'hit': False},\n",
       "     {'score': 0.7285716533660889, 'answer': 'brisk', 'hit': False},\n",
       "     {'score': 0.7226551175117493, 'answer': 'slow', 'hit': False},\n",
       "     {'score': 0.7142457962036133, 'answer': 'snail', 'hit': False},\n",
       "     {'score': 0.706622302532196, 'answer': 'snails', 'hit': False},\n",
       "     {'score': 0.7058804035186768, 'answer': 'frenetic', 'hit': False}],\n",
       "    'set_exclude': ['apple', 'fast', 'banana'],\n",
       "    'rank': 23,\n",
       "    'landing_b': True,\n",
       "    'landing_b_prime': False,\n",
       "    'landing_a': False,\n",
       "    'landing_a_prime': False,\n",
       "    'similarity predicted to b_prime cosine': 0.6776228547096252,\n",
       "    'similarity a to a_prime cosine': 0.670642763376236,\n",
       "    'similarity a_prime to b_prime cosine': 0.5334567055106163,\n",
       "    'similarity b to b_prime cosine': 0.7612074613571167,\n",
       "    'similarity a to b_prime cosine': 0.5206461418420076,\n",
       "    'distance a to a_prime euclidean': 1.147793173789978,\n",
       "    'distance a_prime to b_prime euclidean': 1.366079568862915,\n",
       "    'distance b to b_prime euclidean': 0.9773280620574951,\n",
       "    'distance a to b_prime euclidean': 1.3847076892852783,\n",
       "    'crowdedness of b_prime': [0.7612074613571167,\n",
       "     0.7461464405059814,\n",
       "     0.7251861095428467,\n",
       "     0.7244290113449097,\n",
       "     0.7233452796936035,\n",
       "     0.721161961555481,\n",
       "     0.7205275893211365,\n",
       "     0.7205029129981995,\n",
       "     0.720267653465271,\n",
       "     0.7182656526565552],\n",
       "    'b in neighbourhood of b_prime': 1,\n",
       "    'b_prime in neighbourhood of b': 4}],\n",
       "  'result': {'cnt_questions_correct': 0,\n",
       "   'cnt_questions_total': 2,\n",
       "   'accuracy': 0.0},\n",
       "  'experiment_setup': {'dataset': {'_base_path': './data/w2v/evaluation/analogy',\n",
       "    'class': 'dataset',\n",
       "    'task': 'analogy',\n",
       "    'language': ['english'],\n",
       "    'name': 'TestAnalogy',\n",
       "    'description': 'Test Analogy Set',\n",
       "    'domain': 'general',\n",
       "    'date': '2016',\n",
       "    'source': 'original',\n",
       "    'project_page': 'http://vecto.space/',\n",
       "    'version': '3.0',\n",
       "    'size': 'small',\n",
       "    'cite': {'title': \"Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't\",\n",
       "     'author': 'Gladkova, Anna and Drozd, Aleksandr and Matsuoka, Satoshi',\n",
       "     'doi': '10.18653/v1/N16-2002',\n",
       "     'url': 'https://www.aclweb.org/anthology/N/N16/N16-2002.pdf',\n",
       "     'booktitle': 'Proceedings of the NAACL-HLT SRW',\n",
       "     'publisher': 'ACL',\n",
       "     'year': 2016,\n",
       "     'pages': '47-54',\n",
       "     'type': 'inproceedings',\n",
       "     'id': 'GladkovaDrozdEtAl_2016'},\n",
       "    '_class': 'vecto.data.base.Dataset'},\n",
       "   'embeddings': {'_class': 'vecto.embeddings.legacy_w2v.ModelW2V',\n",
       "    'normalized': True},\n",
       "   'category': 'category1',\n",
       "   'subcategory': 'subcategory_a.txt',\n",
       "   'task': 'word_analogy',\n",
       "   'default_measurement': 'accuracy',\n",
       "   'method': '3CosAdd',\n",
       "   'uuid': '3fb95144-6d37-43f0-bf86-7dbd1970ed7f',\n",
       "   'timestamp': '2019-04-12T17:45:29.939314'}},\n",
       " {'details': [{'question verbose': \"What is to apple as ['quick'] is to fast\",\n",
       "    'b': 'apple',\n",
       "    'expected answer': ['banana'],\n",
       "    'predictions': [{'score': 0.728181004524231,\n",
       "      'answer': 'lebowski',\n",
       "      'hit': False},\n",
       "     {'score': 0.726725697517395, 'answer': 'midget', 'hit': False},\n",
       "     {'score': 0.7247903347015381, 'answer': 'booty', 'hit': False},\n",
       "     {'score': 0.7216457724571228, 'answer': 'coat', 'hit': False},\n",
       "     {'score': 0.7199649214744568, 'answer': 'sauce', 'hit': False},\n",
       "     {'score': 0.7191541194915771, 'answer': 'horn', 'hit': False}],\n",
       "    'set_exclude': ['fast', 'apple', 'quick'],\n",
       "    'rank': 816,\n",
       "    'landing_b': True,\n",
       "    'landing_b_prime': False,\n",
       "    'landing_a': False,\n",
       "    'landing_a_prime': False,\n",
       "    'similarity predicted to b_prime cosine': 0.6400811225175858,\n",
       "    'similarity a to a_prime cosine': 0.7612074613571167,\n",
       "    'similarity a_prime to b_prime cosine': 0.5334567055106163,\n",
       "    'similarity b to b_prime cosine': 0.670642763376236,\n",
       "    'similarity a to b_prime cosine': 0.5013529043644667,\n",
       "    'distance a to a_prime euclidean': 0.9773280620574951,\n",
       "    'distance a_prime to b_prime euclidean': 1.366079568862915,\n",
       "    'distance b to b_prime euclidean': 1.147793173789978,\n",
       "    'distance a to b_prime euclidean': 1.4122990369796753,\n",
       "    'crowdedness of b_prime': [0.8023819923400879,\n",
       "     0.8016062378883362,\n",
       "     0.7981831431388855,\n",
       "     0.7968431711196899,\n",
       "     0.7955393195152283,\n",
       "     0.7927921414375305,\n",
       "     0.7924220561981201,\n",
       "     0.7920680046081543,\n",
       "     0.7914565801620483,\n",
       "     0.7905268669128418],\n",
       "    'b in neighbourhood of b_prime': 8154,\n",
       "    'b_prime in neighbourhood of b': 486},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to apple as ['banana_missing'] is to apple\"},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to apple as ['banana'] is to apple_missing\"},\n",
       "   {'question verbose': \"What is to fast as ['banana'] is to apple\",\n",
       "    'b': 'fast',\n",
       "    'expected answer': ['quick'],\n",
       "    'predictions': [{'score': 0.7522187829017639,\n",
       "      'answer': 'leisurely',\n",
       "      'hit': False},\n",
       "     {'score': 0.7285716533660889, 'answer': 'brisk', 'hit': False},\n",
       "     {'score': 0.7226551175117493, 'answer': 'slow', 'hit': False},\n",
       "     {'score': 0.7142457962036133, 'answer': 'snail', 'hit': False},\n",
       "     {'score': 0.706622302532196, 'answer': 'snails', 'hit': False},\n",
       "     {'score': 0.7058804035186768, 'answer': 'frenetic', 'hit': False}],\n",
       "    'set_exclude': ['apple', 'fast', 'banana'],\n",
       "    'rank': 23,\n",
       "    'landing_b': True,\n",
       "    'landing_b_prime': False,\n",
       "    'landing_a': False,\n",
       "    'landing_a_prime': False,\n",
       "    'similarity predicted to b_prime cosine': 0.6776228547096252,\n",
       "    'similarity a to a_prime cosine': 0.670642763376236,\n",
       "    'similarity a_prime to b_prime cosine': 0.5334567055106163,\n",
       "    'similarity b to b_prime cosine': 0.7612074613571167,\n",
       "    'similarity a to b_prime cosine': 0.5206461418420076,\n",
       "    'distance a to a_prime euclidean': 1.147793173789978,\n",
       "    'distance a_prime to b_prime euclidean': 1.366079568862915,\n",
       "    'distance b to b_prime euclidean': 0.9773280620574951,\n",
       "    'distance a to b_prime euclidean': 1.3847076892852783,\n",
       "    'crowdedness of b_prime': [0.7612074613571167,\n",
       "     0.7461464405059814,\n",
       "     0.7251861095428467,\n",
       "     0.7244290113449097,\n",
       "     0.7233452796936035,\n",
       "     0.721161961555481,\n",
       "     0.7205275893211365,\n",
       "     0.7205029129981995,\n",
       "     0.720267653465271,\n",
       "     0.7182656526565552],\n",
       "    'b in neighbourhood of b_prime': 1,\n",
       "    'b_prime in neighbourhood of b': 4},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to fast as ['banana_missing'] is to apple\"},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to fast as ['banana'] is to apple_missing\"},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to apple as ['banana'] is to apple\"},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to apple as ['quick'] is to fast\"},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to apple as ['banana'] is to apple_missing\"},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to apple_missing as ['banana'] is to apple\"},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to apple_missing as ['quick'] is to fast\"},\n",
       "   {'rank': -1,\n",
       "    'question verbose': \"What is to apple_missing as ['banana_missing'] is to apple\"}],\n",
       "  'result': {'cnt_questions_correct': 0,\n",
       "   'cnt_questions_total': 14,\n",
       "   'accuracy': 0.0},\n",
       "  'experiment_setup': {'dataset': {'_base_path': './data/w2v/evaluation/analogy',\n",
       "    'class': 'dataset',\n",
       "    'task': 'analogy',\n",
       "    'language': ['english'],\n",
       "    'name': 'TestAnalogy',\n",
       "    'description': 'Test Analogy Set',\n",
       "    'domain': 'general',\n",
       "    'date': '2016',\n",
       "    'source': 'original',\n",
       "    'project_page': 'http://vecto.space/',\n",
       "    'version': '3.0',\n",
       "    'size': 'small',\n",
       "    'cite': {'title': \"Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't\",\n",
       "     'author': 'Gladkova, Anna and Drozd, Aleksandr and Matsuoka, Satoshi',\n",
       "     'doi': '10.18653/v1/N16-2002',\n",
       "     'url': 'https://www.aclweb.org/anthology/N/N16/N16-2002.pdf',\n",
       "     'booktitle': 'Proceedings of the NAACL-HLT SRW',\n",
       "     'publisher': 'ACL',\n",
       "     'year': 2016,\n",
       "     'pages': '47-54',\n",
       "     'type': 'inproceedings',\n",
       "     'id': 'GladkovaDrozdEtAl_2016'},\n",
       "    '_class': 'vecto.data.base.Dataset'},\n",
       "   'embeddings': {'_class': 'vecto.embeddings.legacy_w2v.ModelW2V',\n",
       "    'normalized': True},\n",
       "   'category': 'category2',\n",
       "   'subcategory': 'subcategory_b.txt',\n",
       "   'task': 'word_analogy',\n",
       "   'default_measurement': 'accuracy',\n",
       "   'method': '3CosAdd',\n",
       "   'uuid': '550d5ae0-62a5-41c3-b83a-0e0ed4a0bff3',\n",
       "   'timestamp': '2019-04-12T17:45:30.005169'}}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy = Analogy()\n",
    "analogy.get_result(embeddings, path_dataset='./data/w2v/evaluation/analogy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этих двух задач, существует множество других. Бенчмарки для них можно найти на сайте http://vecto.space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что еще бывает?\n",
    "\n",
    "## doc2vec\n",
    "\n",
    "Это word2vec с дополнительной меткой id документа, которая представляет собой отдельный вектор. Вектора id документа и всех слов в нем конкатенируются / усредняются, и таким образом получается вектор документа.\n",
    "\n",
    "Также реализован в `gensim`: `gensim.models.doc2vec`.\n",
    "\n",
    "\n",
    "![img](img/w2v_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "FastText использует не только эмбеддинги слов, но и эмбеддинги n-грамов. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки.\n",
    "\n",
    "* [Статья](https://aclweb.org/anthology/Q17-1010)\n",
    "* [Сайт](https://fasttext.cc/)\n",
    "* [Тьюториал](https://fasttext.cc/docs/en/support.html)\n",
    "* [Вектора для 157 языков](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "* [Вектора, обученные на википедии](https://fasttext.cc/docs/en/pretrained-vectors.html) (отдельно для 294 разных языков)\n",
    "* [Репозиторий](https://github.com/facebookresearch/fasttext)\n",
    "\n",
    "Есть библиотека `fasttext` для питона (с готовыми моделями можно работать и через `gensim`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "Эмбеддинги от стэнфордской NLP-лаборатории. Похожи на word2vec, но в отличие от него GloVe строит матрицу совместной встречаемости слов и учится на ней, т.е. это гибридный метод. [Вот тут](https://www.quora.com/How-is-GloVe-different-from-word2vec) очень хорошо написано об их отличиях.\n",
    "\n",
    "* [Статья](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "* [Сайт](https://nlp.stanford.edu/projects/glove/) (ссылки на модели прямо на главной)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE\n",
    "\n",
    "Модель, основанная на кодировании BPE (Byte Pair Encoding). Это один из способов представления текста, в котором мы используем в качестве токена не слова или символы, а наборы символов в зависимости от глубины кодирования. Предобученные BPE-эмбеддинги для разных языков можно свободно скачивать.\n",
    "\n",
    "* [Статья №1](https://arxiv.org/pdf/1508.07909.pdf) \n",
    "* [Статья №2](https://aclweb.org/anthology/L18-1473)\n",
    "* [Сайт](https://nlp.h-its.org/bpemb/)\n",
    "* [Репозиторий](https://github.com/bheinzerling/bpemb) библиотеки `bpemb` от авторов второй статьи.\n",
    "\n",
    "Скажем, мы хотим закодировать aaabdaaabac. В этой последовательности сочетание \"aa\" встречается наиболее часто. Мы можем \"слить\" эти буквы вместе и рассматривать их как отдельный символ. И так далее итеративно.  Чем больше размер словаря, тем длиннее будут входящие в него единицы: условно, при объеме словаря = 100 текст разобьется на элементы из 2 символов, а при объеме = 500 — на элементы из 5 символов. Выглядит примерно вот так:\n",
    "\n",
    "![bpe1](img/bpe1.jpg)\n",
    "\n",
    "![bpe2](img/bpe2.jpg)\n",
    "\n",
    "А вот 2D-визуализация для BPE-эмбеддингов для русского.\n",
    "\n",
    "![bpe](https://nlp.h-its.org/bpemb/ru/ru.wiki.bpe.emb.vs100000.d100.png)\n",
    "\n",
    "\n",
    "А еще можно посмотреть ее [интерактивную визуализацию](http://projector.tensorflow.org/?config=https://nlp.h-its.org/bpemb/ru/projector.config.json) в TensorFlow Projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400869/400869 [00:00<00:00, 1679525.69B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d50.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1924908/1924908 [00:00<00:00, 2761984.94B/s]\n",
      "2019-09-05 13:10:39,166 : INFO : loading projection weights from /Users/sithcom/.cache/bpemb/en/en.wiki.bpe.vs10000.d50.w2v.bin\n",
      "2019-09-05 13:10:39,283 : INFO : loaded (10000, 50) matrix from /Users/sithcom/.cache/bpemb/en/en.wiki.bpe.vs10000.d50.w2v.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁through', '▁the', '▁looking', '-', 'gl', 'ass', '▁by', '▁lewis', '▁car', 'roll', '▁chapter', '▁i', '▁looking', '-', 'gl', 'ass', '▁house', '▁one', '▁thing', '▁was', '▁certain', '▁that', '▁the', '▁white', '▁kit', 'ten', '▁had', '▁had', '▁nothing', '▁to', '▁do', '▁with', '▁it', '▁it', '▁was', '▁the', '▁black', '▁kit', 'ten', '’', 's', '▁fault', '▁entirely']\n"
     ]
    }
   ],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "bpemb_en = BPEmb(lang='en', dim=50)\n",
    "encoded_text = bpemb_en.encode(' '.join(clean_sents[0]))\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Mover's Distance \n",
    "\n",
    "\n",
    "![img](https://raw.githubusercontent.com/mkusner/wmd/master/fig1.png)\n",
    "\n",
    "\n",
    "Формулировка задачи определения сходства между двумя предложениями как задачи транспортной задачи:\n",
    "\n",
    "1. Пусть $X \\in \\mathbb{R}^{d \\times n}$ – матрица эмбеддингов,  $d$ – размерность эмбеддинга, $n$ - количество слов;\n",
    "2. Вектор-документ в векторной модели: $d \\in \\mathbb{R}^n$ состоит из $c_i = \\texttt{count}(word_i, doc)$\n",
    "3. Нормированный вектор-документ: $d_i = \\frac{c_i}{\\sum_i c_i}$\n",
    "4. Расстояние между словами: $\\texttt{cost}(word_i, word_j) = ||x_i - x_j||_2$\n",
    "\n",
    "Дано два документа, $d, d'$. Пусть  $T \\in \\mathbb{R}^{n \\times n}$, $T_{ij} \\ge 0$ – матрица потока показывает расстояния от каждого слова $d$ до $d'$.\n",
    "\n",
    "Транспортная задача:\n",
    "\n",
    "$\\min_{T \\ge 0} \\sum_{i,j}^n T_{ij}\\texttt{cost}(word_i, word_j) $\n",
    "\n",
    "при условии:\n",
    "\n",
    "$\\sum_{j} T_{ij} = d_i$\n",
    "\n",
    "$\\sum_{i} T_{ij} = d'_j$.\n",
    "\n",
    "Задача решается средствами линейного программирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyemd\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/b1/713de7261a0062ce41c4e2caaa16fe033890fd961b70d637c20951a1c7cf/pyemd-0.5.1-cp36-cp36m-macosx_10_13_x86_64.whl (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 513kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pyemd) (1.14.2)\n",
      "Installing collected packages: pyemd\n",
      "Successfully installed pyemd-0.5.1\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pyemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "from gensim.similarities import WmdSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./data/w2v/movie_reviews.model\"\n",
    "\n",
    "model = word2vec.Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance between s1 and s2 = 0.4699\n",
      "distance between s1 and s3 = 0.4599\n",
      "distance between s2 and s3 = 0.4934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
      "  \"\"\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "s1 = 'The president greets the press in Chicago'\n",
    "s2 = 'Obama speaks to the media in Illinois'\n",
    "s3 = 'Apple releases new laptop model'\n",
    "\n",
    "distance = model.wmdistance(s1, s2)\n",
    "print ('distance between s1 and s2 = %.4f' % distance)\n",
    "\n",
    "distance = model.wmdistance(s1, s3)\n",
    "print ('distance between s1 and s3 = %.4f' % distance)\n",
    "\n",
    "distance = model.wmdistance(s2, s3)\n",
    "print ('distance between s2 and s3 = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание\n",
    "\n",
    "Скачайте [газетный корпус](https://www.dropbox.com/s/7mzom3urqgzyfwa/major_papers.rar?dl=0) (новости из крупных газет на русском языке в xml) или [корпус региональной прессы](https://www.dropbox.com/s/d2gl5atnp7nmgfv/regional_papers.rar?dl=0) (тоже на русском языке, в plain text) и обучите на нем модель word2vec. Можете использовать как оригинальные тексты, так и лемматизированный корпус. \n",
    "\n",
    "**NB!** Нужны данные из папок, метаданные в csv-таблицах не понадобятся.\n",
    "\n",
    "1. Найдите по 5 ближайших слов к словам \"город\", \"деревня\", \"спорт\", \"бизнес\", \"Россия\", \"происшествие\", \"река\", \"озеро\", \"море\", \"горы\", \"депутат\", \"север\", \"юг\", \"кавказ\", \"сибирь\", \"газпром\". Учтите, что слова может не быть в модели.\n",
    "2. Посчитайте семантическую близкость слов \"театр\" и \"кино\", \"Владивосток\" и \"Москва\", \"церковь\" и \"государство\", \"культура\" и \"отдых\", \"преступление\" и \"наказание\".\n",
    "3. Решите примеры:\n",
    "        * москва + екатеринбург - собянин\n",
    "        * спартак - москва + санкт-петербург\n",
    "        * иркутск - байкал + сочи\n",
    "        * татарстан - татарский + бурятия\n",
    "        * чай - лимон + кофе\n",
    "        * авиакомпания - аэрофлот + ржд\n",
    "4. Найдите лишнее, попробуйте интерпретировать результаты\n",
    "        * магазин, супермаркет, рынок, тц\n",
    "        * теннис, хоккей, футбол, дзюдо\n",
    "        * кошка, собака, попугай, кролик\n",
    "        * коми, дагестан, башкирия, камчатка\n",
    "\n",
    "Сохраните модель, она вам еще понадобится!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
