{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Кодирование производится с помощью BiLSTM (не принципиально)\n",
    "* Веса $a_i$ обычно в сумме равны $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/attention.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируя веса, можно понимать стратегию получения результата (от каких частей контекста зависела каждая из частей выхода).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример визуализации весов $attention$ в задаче машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/attention_matrix.png\" alt=\"Drawing\" style=\"width: 480px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании BiLSTM каждый вектор $h_j$ хранит информацию о всей последовательности, но в наибольшей степени о $j$-м слове и его соседях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее при текущем выходе $y_{t−1}$ (с вектором $s_{t−1}$) для вектора каждого входного слова $h_j$ считается $a_{tj}$ – вклад в генерацию следующего выходного вектора ($attention$):\n",
    "\n",
    "$$a_{tj} = \\frac {e^{e_{tj}}} {\\sum_k e^{e_{tk}}}$$\n",
    "\n",
    "где $e_{tj} = f (s_{t−1}, h_j )$ – модель выравнивания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "􏰀Модель выравнивания предсказывает то, насколько хорошо соотносятся входное слово в позиции j и выходное в позиции t (обычно это простая модель, например, однослойная сеть)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вход: вектор запроса и несколько пар векторов ключей и значений (ключ и значение обычно совпадают)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multi-head-attention.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для запроса и каждого ключа считается вес (линейный слой)\n",
    "Значения суммируются с этими весами в итоговый вектор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея: обучать несколько $attention$, в надежде, что они станут отвечать за разные признаки слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты пропустим через однослойную сеть, получим на выходе один вектор той же размерности, что и входные вектора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Состоит из encoder и decoder, в каждом используются $multi$-$head\\ attention$ и полносвязные (свёрточные) слои (нет RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого слова в encoder формируется вектор на основе нескольких слоёв $multi$-$head\\ attention$, который передаётся в декодер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "􏰀На основании векторов энкодера, а также выходных векторов для уже обработанных слов получается вектор для текущего слова. В обоих случаях используется $multi$-$head\\ attention$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Positional\\ encoding$ – дополнительный вектор признаков для каждого слова, представляющий собой набор значений синусов и косинусов с разными периодами от позиции слова в предложении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В трансформере и в encoder, и в decoder можно использовать несколько последовательных слоёв $multi$-$head\\ attention$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для большей выразительности и качества добавляют полносвязные слои, а также дропаут, нормализацию по слою и residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важная особенность: обучение модели внутри предложения можно распараллелить, в отличие от RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$BERT\\ (Bidirectional\\ Encoder\\ Representations\\ from\\ Transformers)\\ -$ многослойный двунаправленный transformer-encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель BERT основана на архитектуре Transformer и является усовершенствованием модели GPT от OpenAI.\n",
    "Модель, в отличие от GPT является двунаправленной.\n",
    "\n",
    "Ссылка на оригинальную статью: https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bert_model.png\" alt=\"Drawing\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные параметры модели:\n",
    "* L – количество $transformer$-блоков \n",
    "* H – размер скрытого представления,\n",
    "* A – количество параллельных слоев в $multi$-$head$ $attention$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует две основные модели от Google:\n",
    "\n",
    "* $BERT_{BASE}$: \n",
    "    * L = 12\n",
    "    * H = 768\n",
    "    * A = 12, \n",
    "    * Total = 110M\n",
    "$$$$\n",
    "* $BERT_{LARGE}$: \n",
    "    * L = 24\n",
    "    * H = 1024\n",
    "    * A = 16, \n",
    "    * Total = 340M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучается на двух задачах: \n",
    "* Masked Language Modeling.\n",
    "* Next sentence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от обычного Language Modeling, где предсказывается каждое следующее слово на каждом шаге, в задаче Masked Language Modeling случайные токены заменяются на специальный токен [MASK] и предсказываются только эти \"замаскированные\" токены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80% времени обучения: Заменим слово на [MASK].\n",
    "    * my dog is hairy → my dog is [MASK]\n",
    "* 10% времени обучения: Заменим слово на другое случайное слово \n",
    "    * my dog is hairy → my dog is apple\n",
    "* 10% времени обучения: оставим предложение без изменений. Цель этого - сделать модель смещенной в сторону слова, которое действительно встречалось в корпусе в данном предложении. \n",
    "    * my dog is hairy → my dog is hairy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next sentence prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next sentence prediction $-$ задача бинарной классификации. По паре предложений требуется определить, является ли второе предложение продолжением первого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input =[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\n",
    "    * Label = IsNext\n",
    "$$$$\n",
    "* Input =[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\n",
    "    * Label = NotNext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на эмбеддинги, получаемые с помощью модели BERT. \n",
    "\n",
    "Будем использовать модель $BERT_{base}$ из реализации pytorch_pretrained_bert от HuggingFace. \n",
    "Ссылка на их гитхаб: https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры текстов взяты из туториала (https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 599772.53B/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank. [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "print (marked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('after', 2044)\n",
      "('stealing', 11065)\n",
      "('money', 2769)\n",
      "('from', 2013)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('vault', 11632)\n",
      "(',', 1010)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('robber', 27307)\n",
      "('was', 2001)\n",
      "('seen', 2464)\n",
      "('fishing', 5645)\n",
      "('on', 2006)\n",
      "('the', 1996)\n",
      "('mississippi', 5900)\n",
      "('river', 2314)\n",
      "('bank', 2924)\n",
      "('.', 1012)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print (tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [02:57<00:00, 2298162.71B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 22\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 22\n",
      "Number of layers per token: 12\n"
     ]
    }
   ],
   "source": [
    "# Convert the hidden state embeddings into single token vectors\n",
    "\n",
    "# Holds the list of 12 layer embeddings for each token\n",
    "# Will have the shape: [# tokens, # layers, # features]\n",
    "token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):\n",
    "  \n",
    "  # Holds 12 layers of hidden states for each token \n",
    "    hidden_layers = [] \n",
    "  \n",
    "  # For each of the 12 layers...\n",
    "    for layer_i in range(len(encoded_layers)):\n",
    "    \n",
    "    # Lookup the vector for `token_i` in `layer_i`\n",
    "        vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "        hidden_layers.append(vec)\n",
    "    \n",
    "    token_embeddings.append(hidden_layers)\n",
    "\n",
    "# Sanity check the dimensions:\n",
    "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "print (\"Number of layers per token:\", len(token_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_last_4_layers = [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in token_embeddings] # [number_of_tokens, 3072]\n",
    "\n",
    "summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] # [number_of_tokens, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(encoded_layers[11], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\"), sentence_embedding[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\n"
     ]
    }
   ],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(tokenized_text):\n",
    "    print (i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First fifteen values of 'bank' as in 'bank robber':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1868, -1.5298, -1.3770,  1.0648,  3.1446,  1.4003, -4.2407,  1.3946,\n",
       "        -0.1170, -1.8777,  0.1091, -0.3862,  0.6744,  2.1924, -4.5306])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"First fifteen values of 'bank' as in 'bank robber':\")\n",
    "summed_last_4_layers[10][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First fifteen values of 'bank' as in 'bank vault':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1319, -2.1413, -1.6260,  0.8638,  3.3173,  0.1797, -4.4853,  3.1215,\n",
       "        -0.9740, -3.1780,  0.1046, -1.5481,  0.4758,  1.1703, -4.4859])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"First fifteen values of 'bank' as in 'bank vault':\")\n",
    "summed_last_4_layers[6][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First fifteen values of 'bank' as in 'river bank':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1295, -1.4725, -0.7296, -0.0901,  2.4970,  0.5330,  0.9742,  5.1834,\n",
       "        -1.0692, -1.5941,  1.9261,  0.7119, -0.9809,  1.2127, -2.9812])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"First fifteen values of 'bank' as in 'river bank':\")\n",
    "summed_last_4_layers[19][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"river bank\"\n",
    "different_bank = cosine_similarity(summed_last_4_layers[10].reshape(1,-1), summed_last_4_layers[19].reshape(1,-1))[0][0]\n",
    "\n",
    "# Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"bank vault\" \n",
    "same_bank = cosine_similarity(summed_last_4_layers[10].reshape(1,-1), summed_last_4_layers[6].reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'bank' as in 'bank robber' to 'bank' as in 'bank vault': 0.9456751\n"
     ]
    }
   ],
   "source": [
    "print (\"Similarity of 'bank' as in 'bank robber' to 'bank' as in 'bank vault':\",  same_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'bank' as in 'bank robber' to 'bank' as in 'river bank': 0.6797334\n"
     ]
    }
   ],
   "source": [
    "print (\"Similarity of 'bank' as in 'bank robber' to 'bank' as in 'river bank':\",  different_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
