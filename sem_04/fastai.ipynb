{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jrp7g1APLvU3"
   },
   "source": [
    "# Fastai "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6E6EessLvU9"
   },
   "source": [
    "[Fastai](https://github.com/fastai/fastai) - фреймворк для обучения моделей (NLP, computer vision, collaborative filtering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQa7vcz3LvU-"
   },
   "source": [
    "Курс по NLP от fast.ai: https://www.fast.ai/2019/07/08/fastai-nlp/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DL71KS-BAlPn"
   },
   "source": [
    "## Классификация с RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vo3MN-xLvVA"
   },
   "source": [
    "### Источники"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYK_pq7PLvVD"
   },
   "source": [
    "Рассмотрим задачу fine-tuning модели RoBERTa для классификации. Будем использовать методы для обработки датасетов и обучения моделей из Fastai и предобученную модель RoBERTa из [fairseq](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIs8xN59LvVC"
   },
   "source": [
    "https://medium.com/@devkosal/using-roberta-with-fastai-for-nlp-7ed3fed21f6c <br>\n",
    "https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4hcjbWhLvVF"
   },
   "source": [
    "[RoBERTa](https://arxiv.org/abs/1907.11692) - модель от Facebook AI на основе BERT. Архитектура такая же, но\n",
    "* RoBERTa обучалась дольше на большем количестве данных (кроме `BOOKCORPUS`, использовались ещё `CC-NEWS`, `OPENWEBTEXT` (данные с reddit) и `STORIES`)\n",
    "* В отличие от BERT модель обучалась только на задаче masked language modelling (без обучения предсказанию следующего предложения) с нефиксированным максированием. \n",
    "* С настройкой оптимизации (tuning Adam epsilon term)\n",
    "* С батчами большего размера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "rnpsvjkYMFcH",
    "outputId": "386cc212-0a73-4a15-e77d-204ac6d5d21d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.59)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.7)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (19.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.17.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.7.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.0+cu100)\n",
      "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.2.1)\n",
      "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.9)\n",
      "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.21)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.25.3)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.4.1+cu100)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.9.11)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (1.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.6.1)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (7.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.0)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.4)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.3.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (41.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.0.18->fastai) (4.28.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "DPtLxpZnMZne",
    "outputId": "749bfef2-89f8-4b5c-b86b-bbed335fd864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "\r",
      "\u001b[K     |█▉                              | 10kB 32.6MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 20kB 6.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 30kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 40kB 5.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 51kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 61kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 71kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 81kB 10.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 92kB 11.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 102kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 112kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 122kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 133kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 143kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 153kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 163kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 174kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 184kB 9.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.17.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 63.1MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
      "\u001b[K     |████████████████████████████████| 860kB 73.6MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 69.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.3.0+cu100)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.10.7)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.0)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.7 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.13.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->pytorch-transformers) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->pytorch-transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=a963d1b144a9b7b797f849e26e66d354198e607eaea8187513defea52e0f4edc\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, sacremoses, sentencepiece, pytorch-transformers\n",
      "Successfully installed pytorch-transformers-1.2.0 regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:53.112411Z",
     "start_time": "2019-09-04T13:50:52.024302Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lgNOema0LvVH"
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.metrics import *\n",
    "from pytorch_transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7mqZ2KabTk2"
   },
   "source": [
    "## Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:53.119474Z",
     "start_time": "2019-09-04T13:50:53.113824Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eciJQ8gfLvVM"
   },
   "outputs": [],
   "source": [
    "# Creating a config object to store task specific information\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    testing=False,\n",
    "    seed = 2019,\n",
    "    roberta_model_name='roberta-base', # can also be exchnaged with roberta-large \n",
    "    max_lr=1e-5,\n",
    "    epochs=1,\n",
    "    use_fp16=False,\n",
    "    bs=4, \n",
    "    max_seq_len=256, \n",
    "    num_labels = 2,\n",
    "    hidden_dropout_prob=.05,\n",
    "    hidden_size=768, # 1024 for roberta-large\n",
    "    start_tok = \"<s>\",\n",
    "    end_tok = \"</s>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-u-rAVdbYcU"
   },
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eKyDbP2bbwE"
   },
   "source": [
    "Используем IMDB датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "18anDiK5bima",
    "outputId": "46af7f5a-510b-499d-e296-19918566e167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-10 13:16:19--  https://raw.githubusercontent.com/devkosal/fastai_roberta/master/fastai_roberta_imdb/imdb_dataset.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 66212309 (63M) [text/plain]\n",
      "Saving to: ‘imdb_dataset.csv’\n",
      "\n",
      "\r",
      "imdb_dataset.csv      0%[                    ]       0  --.-KB/s               \r",
      "imdb_dataset.csv     84%[===============>    ]  53.40M   267MB/s               \r",
      "imdb_dataset.csv    100%[===================>]  63.14M   282MB/s    in 0.2s    \n",
      "\n",
      "2019-11-10 13:16:20 (282 MB/s) - ‘imdb_dataset.csv’ saved [66212309/66212309]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/devkosal/fastai_roberta/master/fastai_roberta_imdb/imdb_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:53.557101Z",
     "start_time": "2019-09-04T13:50:53.122045Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DiYpFrrRLvVQ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imdb_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:53.562452Z",
     "start_time": "2019-09-04T13:50:53.558919Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GzYUKzk9LvVV",
    "outputId": "4a64131a-5224-4422-d917-144eead4b0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "if config.testing: df = df[:5000]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:53.583364Z",
     "start_time": "2019-09-04T13:50:53.564068Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Ad5ABb2OLvVb",
    "outputId": "aef030b9-abf5-499c-d1a3-22c48c6d1631"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:53.591663Z",
     "start_time": "2019-09-04T13:50:53.585091Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "neWnysZdLvVg"
   },
   "outputs": [],
   "source": [
    "feat_cols = \"review\"\n",
    "label_cols = \"sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dadiP0qhWbWg"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aNO_hBEvUYh4",
    "outputId": "8c47d60e-b9c6-455e-fa5e-fc88c4ee8b25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898823/898823 [00:00<00:00, 2100422.43B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 1345577.80B/s]\n"
     ]
    }
   ],
   "source": [
    "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8E-AdweYECe"
   },
   "source": [
    "RoBERTa использует специальные символы начала и конца предложения: \\<s\\> и \\</s\\>. <br> Для совместимости с токенайзером из fastai нужно задать обертку над RobertaTokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:53.665067Z",
     "start_time": "2019-09-04T13:50:53.657647Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xcCz2kKtLvVm"
   },
   "outputs": [],
   "source": [
    "class FastAiRobertaTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n",
    "    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len \n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self \n",
    "    def tokenizer(self, t:str) -> List[str]: \n",
    "        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n",
    "        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:54.632414Z",
     "start_time": "2019-09-04T13:50:54.275831Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "D3O9ZuWZLvVs"
   },
   "outputs": [],
   "source": [
    "# create fastai tokenizer for roberta\n",
    "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n",
    "                             pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L0zSwBMKtgCe",
    "outputId": "00fa2e55-29e2-48bd-d89f-26db44a9b4b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Ġthe', 'Ġmovie', 'Ġwas', 'Ġgreat', '</s>']"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastAiRobertaTokenizer(roberta_tok).tokenizer('the movie was great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMZDa0BnsBMi"
   },
   "source": [
    "Создадим словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:55.213017Z",
     "start_time": "2019-09-04T13:50:55.095284Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Df6FGmLpLvVw"
   },
   "outputs": [],
   "source": [
    "# create fastai vocabulary for roberta\n",
    "path = Path()\n",
    "roberta_tok.save_vocabulary(path)\n",
    "\n",
    "with open('vocab.json', 'r') as f:\n",
    "    roberta_vocab_dict = json.load(f)\n",
    "    \n",
    "fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:55.920906Z",
     "start_time": "2019-09-04T13:50:55.914824Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "J2SvON6QLvV0"
   },
   "outputs": [],
   "source": [
    "# Setting up pre-processors\n",
    "class RobertaTokenizeProcessor(TokenizeProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "class RobertaNumericalizeProcessor(NumericalizeProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n",
    "\n",
    "\n",
    "def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    \"\"\"\n",
    "    Constructing preprocessors for Roberta\n",
    "    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n",
    "    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n",
    "    \"\"\"\n",
    "    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQjI8af7LvV6"
   },
   "source": [
    "## DataBunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHD6-EP7uXJf"
   },
   "source": [
    "Класс DataBunch для предварительной обработки и подготовки данных для модели Roberta. Создает батчи с учетом длины последовательностей, добавляет padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fastai](fastai.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:57.966052Z",
     "start_time": "2019-09-04T13:50:57.959106Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zkCarOnwLvV8"
   },
   "outputs": [],
   "source": [
    "# Creating a Roberta specific DataBunch class\n",
    "class RobertaDataBunch(TextDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training Roberta\"\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n",
    "               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n",
    "               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n",
    "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:50:58.992534Z",
     "start_time": "2019-09-04T13:50:58.986024Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aiAnEErrLvWA"
   },
   "outputs": [],
   "source": [
    "class RobertaTextList(TextList):\n",
    "    _bunch = RobertaDataBunch\n",
    "    _label_cls = TextList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:51:22.079406Z",
     "start_time": "2019-09-04T13:51:00.215358Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_hqJMe6ELvWE"
   },
   "outputs": [],
   "source": [
    "# loading the tokenizer and vocab processors\n",
    "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
    "\n",
    "# creating our databunch \n",
    "data = RobertaTextList.from_df(df, \".\", cols=feat_cols, processor=processor) \\\n",
    "    .split_by_rand_pct(seed=config.seed) \\\n",
    "    .label_from_df(cols=label_cols,label_cls=CategoryList) \\\n",
    "    .databunch(bs=config.bs, pad_first=False, pad_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "gBQ8_hom0myN",
    "outputId": "f5faf5d2-7e5e-4dd9-d355-33b88841fad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaDataBunch;\n",
       "\n",
       "Train: LabelList (40000 items)\n",
       "x: RobertaTextList\n",
       "<s> ĠA Ġwonderful Ġlittle Ġproduction . Ġ< br Ġ/ >< br Ġ/> The Ġfilming Ġtechnique Ġis Ġvery Ġun assuming - Ġvery Ġold - time - BBC Ġfashion Ġand Ġgives Ġa Ġcomforting , Ġand Ġsometimes Ġdiscomfort ing , Ġsense Ġof Ġrealism Ġto Ġthe Ġentire Ġpiece . Ġ< br Ġ/ >< br Ġ/> The Ġactors Ġare Ġextremely Ġwell Ġchosen - ĠMichael ĠSheen Ġnot Ġonly Ġ\" has Ġgot Ġall Ġthe Ġpol ari \" Ġbut Ġhe Ġhas Ġall Ġthe Ġvoices Ġdown Ġpat Ġtoo ! ĠYou Ġcan Ġtruly Ġsee Ġthe Ġseamless Ġediting Ġguided Ġby Ġthe Ġreferences Ġto ĠWilliams ' Ġdiary Ġentries , Ġnot Ġonly Ġis Ġit Ġwell Ġworth Ġthe Ġwatching Ġbut Ġit Ġis Ġa Ġterrific ly Ġwritten Ġand Ġperformed Ġpiece . ĠA Ġmaster ful Ġproduction Ġabout Ġone Ġof Ġthe Ġgreat Ġmaster 's Ġof Ġcomedy Ġand Ġhis Ġlife . Ġ< br Ġ/ >< br Ġ/> The Ġrealism Ġreally Ġcomes Ġhome Ġwith Ġthe Ġlittle Ġthings : Ġthe Ġfantasy Ġof Ġthe Ġguard Ġwhich , Ġrather Ġthan Ġuse Ġthe Ġtraditional Ġ' dream ' Ġtechniques Ġremains Ġsolid Ġthen Ġdisappears . ĠIt Ġplays Ġon Ġour Ġknowledge Ġand Ġour Ġsenses , Ġparticularly Ġwith Ġthe Ġscenes Ġconcerning ĠOr ton Ġand ĠHall i well Ġand Ġthe Ġsets Ġ( particularly Ġof Ġtheir Ġflat Ġwith ĠHall i well 's Ġmur als Ġdecor ating Ġevery Ġsurface ) Ġare Ġterribly Ġwell Ġdone . </s>,<s> ĠI Ġthought Ġthis Ġwas Ġa Ġwonderful Ġway Ġto Ġspend Ġtime Ġon Ġa Ġtoo Ġhot Ġsummer Ġweekend , Ġsitting Ġin Ġthe Ġair Ġconditioned Ġtheater Ġand Ġwatching Ġa Ġlight - hearted Ġcomedy . ĠThe Ġplot Ġis Ġsimplistic , Ġbut Ġthe Ġdialogue Ġis Ġwitty Ġand Ġthe Ġcharacters Ġare Ġlik able Ġ( even Ġthe Ġwell Ġbread Ġsuspected Ġserial Ġkiller ). ĠWhile Ġsome Ġmay Ġbe Ġdisappointed Ġwhen Ġthey Ġrealize Ġthis Ġis Ġnot ĠMatch ĠPoint Ġ2 : ĠRisk ĠAddiction , ĠI Ġthought Ġit Ġwas Ġproof Ġthat ĠWoody ĠAllen Ġis Ġstill Ġfully Ġin Ġcontrol Ġof Ġthe Ġstyle Ġmany Ġof Ġus Ġhave Ġgrown Ġto Ġlove .< br Ġ/ >< br Ġ/> This Ġwas Ġthe Ġmost ĠI 'd Ġlaughed Ġat Ġone Ġof ĠWoody 's Ġcomed ies Ġin Ġyears Ġ( d are ĠI Ġsay Ġa Ġdecade ?). ĠWhile ĠI 've Ġnever Ġbeen Ġimpressed Ġwith ĠScarlet ĠJoh anson , Ġin Ġthis Ġshe Ġmanaged Ġto Ġtone Ġdown Ġher Ġ\" se xy \" Ġimage Ġand Ġjumped Ġright Ġinto Ġa Ġaverage , Ġbut Ġspirited Ġyoung Ġwoman .< br Ġ/ >< br Ġ/> This Ġmay Ġnot Ġbe Ġthe Ġcrown Ġjewel Ġof Ġhis Ġcareer , Ġbut Ġit Ġwas Ġw itt ier Ġthan Ġ\" Dev il ĠW ears ĠPr ada \" Ġand Ġmore Ġinteresting Ġthan Ġ\" Super man \" Ġa Ġgreat Ġcomedy Ġto Ġgo Ġsee Ġwith Ġfriends . </s>,<s> ĠBasically Ġthere 's Ġa Ġfamily Ġwhere Ġa Ġlittle Ġboy Ġ( Jake ) Ġthinks Ġthere 's Ġa Ġzombie Ġin Ġhis Ġcloset Ġ& Ġhis Ġparents Ġare Ġfighting Ġall Ġthe Ġtime .< br Ġ/ >< br Ġ/> This Ġmovie Ġis Ġslower Ġthan Ġa Ġsoap Ġopera ... Ġand Ġsuddenly , ĠJake Ġdecides Ġto Ġbecome ĠRam bo Ġand Ġkill Ġthe Ġzombie .< br Ġ/ >< br Ġ/> OK , Ġfirst Ġof Ġall Ġwhen Ġyou 're Ġgoing Ġto Ġmake Ġa Ġfilm Ġyou Ġmust ĠDec ide Ġif Ġits Ġa Ġthriller Ġor Ġa Ġdrama ! ĠAs Ġa Ġdrama Ġthe Ġmovie Ġis Ġwatch able . ĠParents Ġare Ġdivor cing Ġ& Ġarguing Ġlike Ġin Ġreal Ġlife . ĠAnd Ġthen Ġwe Ġhave ĠJake Ġwith Ġhis Ġcloset Ġwhich Ġtotally Ġruins Ġall Ġthe Ġfilm ! ĠI Ġexpected Ġto Ġsee Ġa ĠB OO GE Y MAN Ġsimilar Ġmovie , Ġand Ġinstead Ġi Ġwatched Ġa Ġdrama Ġwith Ġsome Ġmeaningless Ġthriller Ġspots .< br Ġ/ >< br Ġ/> 3 Ġout Ġof Ġ10 Ġjust Ġfor Ġthe Ġwell Ġplaying Ġparents Ġ& Ġdescent Ġdialog s . ĠAs Ġfor Ġthe Ġshots Ġwith ĠJake : Ġjust Ġignore Ġthem . </s>,<s> ĠPet ter ĠMatte i 's Ġ\" Love Ġin Ġthe ĠTime Ġof ĠMoney \" Ġis Ġa Ġvisually Ġstunning Ġfilm Ġto Ġwatch . ĠMr . ĠMatte i Ġoffers Ġus Ġa Ġvivid Ġportrait Ġabout Ġhuman Ġrelations . ĠThis Ġis Ġa Ġmovie Ġthat Ġseems Ġto Ġbe Ġtelling Ġus Ġwhat Ġmoney , Ġpower Ġand Ġsuccess Ġdo Ġto Ġpeople Ġin Ġthe Ġdifferent Ġsituations Ġwe Ġencounter . Ġ< br Ġ/ >< br Ġ/> This Ġbeing Ġa Ġvariation Ġon Ġthe ĠArthur ĠSchn itz ler 's Ġplay Ġabout Ġthe Ġsame Ġtheme , Ġthe Ġdirector Ġtransfers Ġthe Ġaction Ġto Ġthe Ġpresent Ġtime ĠNew ĠYork Ġwhere Ġall Ġthese Ġdifferent Ġcharacters Ġmeet Ġand Ġconnect . ĠEach Ġone Ġis Ġconnected Ġin Ġone Ġway , Ġor Ġanother Ġto Ġthe Ġnext Ġperson , Ġbut Ġno Ġone Ġseems Ġto Ġknow Ġthe Ġprevious Ġpoint Ġof Ġcontact . ĠSty lish ly , Ġthe Ġfilm Ġhas Ġa Ġsophisticated Ġluxurious Ġlook . ĠWe Ġare Ġtaken Ġto Ġsee Ġhow Ġthese Ġpeople Ġlive Ġand Ġthe Ġworld Ġthey Ġlive Ġin Ġtheir Ġown Ġhabitat .< br Ġ/ >< br Ġ/> The Ġonly Ġthing Ġone Ġgets Ġout Ġof Ġall Ġthese Ġsouls Ġin Ġthe Ġpicture Ġis Ġthe Ġdifferent Ġstages Ġof Ġloneliness Ġeach Ġone Ġinhab its . ĠA Ġbig Ġcity Ġis Ġnot Ġexactly Ġthe Ġbest Ġplace Ġin Ġwhich Ġhuman Ġrelations Ġfind Ġsincere Ġfulfillment , Ġas Ġone Ġdiscern s Ġis Ġthe Ġcase Ġwith Ġmost Ġof Ġthe Ġpeople Ġwe Ġencounter .< br Ġ/ >< br Ġ/> The Ġacting Ġis Ġgood Ġunder ĠMr . ĠMatte i 's Ġdirection . ĠSteve ĠBus ce mi , ĠRos ario ĠDawson , ĠCarol ĠKane , ĠMichael ĠImper iol </s>,<s> ĠI Ġsure Ġwould Ġlike Ġto Ġsee Ġa Ġresurrection Ġof Ġa Ġup Ġdated ĠSeah unt Ġseries Ġwith Ġthe Ġtech Ġthey Ġhave Ġtoday Ġit Ġwould Ġbring Ġback Ġthe Ġkid Ġexcitement Ġin Ġme . I Ġgrew Ġup Ġon Ġblack Ġand Ġwhite ĠTV Ġand ĠSeah unt Ġwith ĠGun sm oke Ġwere Ġmy Ġhero 's Ġevery Ġweek . You Ġhave Ġmy Ġvote Ġfor Ġa Ġcomeback Ġof Ġa Ġnew Ġsea Ġhunt . We Ġneed Ġa Ġchange Ġof Ġpace Ġin ĠTV Ġand Ġthis Ġwould Ġwork Ġfor Ġa Ġworld Ġof Ġunder Ġwater Ġadventure . Oh Ġby Ġthe Ġway Ġthank Ġyou Ġfor Ġan Ġoutlet Ġlike Ġthis Ġto Ġview Ġmany Ġviewpoints Ġabout ĠTV Ġand Ġthe Ġmany Ġmovies . So Ġany Ġo le Ġway ĠI Ġbelieve ĠI 've Ġgot Ġwhat ĠI Ġwanna Ġsay . Would Ġbe Ġnice Ġto Ġread Ġsome Ġmore Ġplus Ġpoints Ġabout Ġsea Ġhunt . If Ġmy Ġrh ymes Ġwould Ġbe Ġ10 Ġlines Ġwould Ġyou Ġlet Ġme Ġsubmit , or Ġleave Ġme Ġout Ġto Ġbe Ġin Ġdoubt Ġand Ġhave Ġme Ġto Ġquit , If Ġthis Ġis Ġso Ġthen ĠI Ġmust Ġgo Ġso Ġlets Ġdo Ġit . </s>\n",
       "y: CategoryList\n",
       "positive,positive,negative,positive,positive\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (10000 items)\n",
       "x: RobertaTextList\n",
       "<s> ĠFor Ġa Ġfilm Ġby Ġa Ġfirst Ġtime Ġdirector , Ġthis Ġis Ġa Ġsimply Ġamazing Ġlittle Ġfilm , Ġthough ĠI Ġhave Ġto Ġadmit Ġthat Ġit 's Ġa Ġbit Ġhard Ġto Ġwatch Ġand Ġwill Ġprobably Ġoffend Ġthe Ġsensibilities Ġof Ġmany Ġwho Ġwatch Ġit , Ġas Ġthe Ġfilm Ġtakes Ġa Ġdefinite Ġstand Ġin Ġfavor Ġof Ġassisted Ġsuicide . ĠHowever , Ġwhat ĠI Ġloved Ġabout Ġthe Ġfilm Ġis Ġthat ĠI Ġhad Ġno Ġidea Ġthis Ġis Ġwhat Ġthe Ġfilm Ġwas Ġabout Ġuntil Ġthe Ġfilm Ġwas Ġmore Ġthan Ġhalf Ġcomplete Ġand Ġby Ġthen ĠI Ġwas Ġalready Ġhooked .< br Ġ/ >< br Ġ/> In Ġaddition Ġto Ġthe Ġexcellent Ġway Ġthe Ġsubject Ġwas Ġintroduced , ĠI Ġwas Ġamazed Ġat Ġthe Ġvery Ġprofessional Ġstyle Ġof Ġthe Ġfilm -- with Ġsome Ġof Ġthe Ġbest Ġcinem atography ĠI 've Ġever Ġseen Ġin Ġa Ġshort Ġfilm . ĠPlus , Ġthis Ġcamera Ġwork Ġwas Ġcombined Ġso Ġwell Ġwith Ġmusic Ġand Ġimages Ġthat Ġmade Ġthe Ġfilm Ġjust Ġbeautiful .< br Ġ/ >< br Ġ/> As ĠI Ġsaid , Ġthis Ġis Ġa Ġcontroversial Ġfilm . ĠI Ġmyself Ġstruggle Ġwith Ġwhat ĠI Ġthink Ġabout Ġthis Ġdebate , Ġbut Ġthe Ġfilm Ġmanaged Ġto Ġmake Ġa Ġstatement Ġin Ġsuch Ġa Ġliter ate Ġand Ġintelligent Ġway , ĠI Ġreally Ġhave Ġto Ġrespect Ġthe Ġteam Ġwho Ġmade Ġthis Ġfilm . </s>,<s> Ġ\" G es pen ster \" ĠQuestion Ġof Ġto Ġbe Ġcool Ġin Ġthe ĠGerman Ġcinema < br Ġ/ >< br Ġ/> There Ġare Ġnot Ġmany ĠGerman Ġfilms Ġin Ġthe Ġlast Ġten Ġyears , Ġwho Ġhave Ġmade Ġme Ġso Ġinterest . ĠYes , Ġthe Ġproblem Ġof Ġthe Ġmost ĠGerman Ġfilms Ġare Ġin Ġthis Ġfilm Ġ\" G es pen ster \" Ġtoo . ĠHe Ġis Ġon Ġsome Ġplaces Ġto Ġunc ooked Ġto Ġbe Ġgood Ġto Ġsee . ĠSpecial Ġthe Ġfigure Ġof ĠT oni Ġ( Sab ine ĠT iment o ) Ġis Ġtoo Ġcool . ĠBut Ġthats Ġis Ġin ĠGerman Ġfilms Ġalways Ġso . ĠEverybody Ġmust Ġto Ġlearns Ġthis Ġcool ne ÃŁ Ġ- Ġis Ġthe Ġrealism Ġin Ġthis Ġfilms . ĠThat s Ġdifficult Ġto Ġunderstand . ĠBut Ġin Ġthis Ġcase Ġit Ġmakes Ġsome Ġsense , Ġbecause Ġshe Ġsteals Ġand Ġshe Ġlies Ġ- Ġshe Ġis Ġthe Ġkind Ġof Ġgirl Ġis Ġbetter Ġyou Ġnever Ġlove Ġit , Ġbecause Ġyou Ġlose Ġit . ĠThat s Ġnot Ġclear Ġfor Ġthe Ġother Ġgirl ĠNina Ġin Ġthis Ġfilm . ĠShe Ġlove Ġher Ġ- Ġand Ġshe Ġwould Ġlose Ġher . ĠBut ĠNina Ġlost Ġeverything . ĠShe Ġwill Ġplay Ġwith Ġsoft Ġemotion Ġand Ġa Ġsad Ġfeeling . ĠThere Ġis Ġno Ġway Ġ- Ġbut Ġyou Ġmust Ġtake Ġit Ġsaid ĠHerbert ĠAch ter bus ch Ġfor Ġtwenty Ġyears . ĠThat s Ġso Ġoften Ġthe Ġway Ġit Ġgoes Ġin ĠGerman Ġfilms . ĠWhy ? ĠNina Ġ( Jul ia ĠHum mer ) Ġis Ġnot Ġinside Ġof Ġthe Ġlaws Ġof Ġsociety Ġ- Ġthe Ġis Ġoutside Ġ- Ġand Ġthere Ġshe Ġhave </s>,<s> ĠWh a - B AM ! ĠSomeone Ġsurely Ġhad Ġfun Ġdev ouring Ġa Ġwhole Ġtruck load Ġof Ġacid - m ush rooms Ġand Ġthen Ġsubsequently Ġscripting Ġthis Ġcrazy Ġexcuse Ġfor Ġa Ġmotion Ġpicture ! ĠWriter ĠHoward ĠCohen Ġexpands Ġthe Ġ\" Sword Ġ& ĠSor cery \" Ġconcept Ġwith Ġa Ġcouple Ġof Ġextra ĠS 's , Ġlike ĠSex , ĠS ill iness , Ġ( more ) ĠSex Ġand ĠShe er ĠStupid ity ! ĠThis Ġisn 't Ġjust Ġa Ġmovie , Ġthis Ġis Ġevery Ġjuvenile Ġper vert 's Ġdreams Ġ& Ġfantasies Ġcome Ġtrue ! Ġ\" Death st alker \" Ġhas Ġit Ġall : Ġblood , Ġviolence , Ġtrolls , Ġfemale Ġmud - w restling , Ġattempted Ġrape , Ġsuccessful Ġrape , Ġlife - sized Ġpigs Ġ(! ), Ġawful Ġhairst yles , Ġhide ously Ġoil ed Ġmuscular Ġbodies , Ġmulti - sexual Ġorg ies , Ġgay Ġwarriors , Ġtournaments - to - the - death , Ġdel ir ious Ġwitches , Ġdism ember ment , Ġlaughable Ġvillains Ġand Ġboobs , Ġboobs , ĠB OOOOOOOO O BI ES !! Ġ\" Death st alker \" Ġliterally Ġwipes Ġthe Ġfloor Ġwith Ġits Ġobvious Ġrole - model Ġ\" Con an : ĠThe ĠBarbarian \" Ġwhen Ġit Ġcomes Ġto Ġterms Ġof Ġchees iness Ġand Ġsheer Ġfl amb oy ance . ĠThe Ġstory Ġis , Ġevidently , Ġof Ġminor Ġimportance . ĠLone Ġand Ġgay Ġ( only Ġhe Ġdoesn 't Ġknow Ġit Ġyet ) Ġwarrior ĠDeath st alker Ġgoes Ġon Ġa Ġmission , Ġas Ġcommanded Ġby Ġan Ġannoying Ġwitch , Ġto Ġgather Ġthe Ġthree Ġnotorious </s>,<s> ĠI Ġwas Ġfairly Ġlost Ġthroughout Ġmost Ġof Ġthis Ġfilm , Ġand ĠI Ġam Ġthe Ġone Ġwho Ġusually Ġunderstands Ġthe Ġworks Ġof Ġsuch Ġenigmatic Ġcinema Ġgreat s Ġas ĠDavid ĠLynch Ġ( Tw in ĠPeaks : ĠFire ĠWalk ĠWith ĠMe ) Ġand ĠDarren ĠAr on of sky Ġ( Pi ). ĠNot Ġto Ġsay Ġthat ĠNorth fork Ġdoesn 't Ġmake Ġsense Ġon Ġsome Ġlevel , Ġit Ġjust Ġdoesn 't Ġcombine Ġto Ġform Ġa Ġwholly Ġcoherent Ġfilm . ĠAs Ġtime Ġpasses Ġfrom Ġwatching Ġthe Ġfilm , Ġits Ġthemes Ġand Ġintentions Ġbecome Ġclearer , Ġbut Ġduring Ġmy Ġinitial Ġviewing , ĠI Ġwas Ġreally Ġconf ounded , Ġand ĠI Ġfind Ġthat Ġthis Ġis Ġthe Ġmajor Ġfault Ġof Ġthe Ġfilm ... its Ġlack Ġof Ġdirection . ĠThe Ġplot Ġcenters Ġon Ġthe Ġtown Ġof ĠNorth fork , ĠMontana Ġin Ġthe Ġyear Ġ1955 . ĠThe Ġtown Ġhas Ġbeen Ġemptied Ġand Ġwill Ġsoon Ġbe Ġflooded Ġto Ġmake Ġway Ġfor Ġthe Ġcreation Ġof Ġa Ġhydro - electric Ġdam . ĠThe Ġmajor Ġproblem Ġis Ġthat Ġnot Ġall Ġof Ġits Ġinhabitants Ġare Ġwilling Ġto Ġbe Ġevacuated Ġand Ġrelocated . ĠA Ġgroup Ġof Ġmen Ġare Ġhired Ġto Ġcoer ce Ġthe Ġremaining Ġresidents Ġout Ġof Ġthe Ġtown Ġbefore Ġit Ġwill Ġbe Ġdrowned , Ġand Ġfor Ġthe Ġmost Ġpart Ġthey Ġsucceed Ġamidst Ġsome Ġfairly Ġodd Ġsituations Ġand Ġtown spe ople . ĠSim ultane ously , Ġthe Ġfilm Ġtells Ġthe Ġstory Ġof ĠIrwin , Ġa Ġvery Ġsick Ġyoung Ġboy Ġ( or Ġis Ġhe Ġa Ġfallen Ġangel ?) Ġwhose Ġadopted Ġparents Ġgave Ġhim Ġback , Ġdue Ġto Ġhis Ġillness , Ġto Ġthe </s>,<s> Ġthis Ġmovie Ġis Ġsuch Ġa Ġmoving , Ġamazing Ġpiece Ġof Ġwork . Ġi Ġsaw Ġit Ġat Ġthe Ġtheater Ġwhen Ġit Ġcame Ġout , Ġbut Ġi Ġwas Ġonly Ġ13 Ġ& Ġdidn 't Ġreally Ġquite Ġ\" get Ġit \"... Ġi Ġsaw Ġit Ġagain Ġwhen Ġi Ġwas Ġ20 Ġ( on Ġvideo Ġof Ġcourse Ġ& Ġi Ġnow Ġown Ġit ) Ġ& Ġwas Ġjust Ġblown Ġaway . ĠSteven ĠSpielberg Ġcreated Ġa Ġwonderful Ġmovie Ġthat Ġkeeps Ġyou Ġwrapped Ġup Ġin Ġit Ġfrom Ġbeginning Ġto Ġend . Ġi Ġhave Ġread Ġthe Ġbook Ġas Ġwell , Ġbut Ġthere Ġis Ġjust Ġsomething Ġabout Ġthe Ġmovie Ġthat Ġreally Ġbrings Ġit Ġto Ġlife . Ġthe Ġcasting , Ġacting , Ġmusic , Ġcost uming , Ġscenery , Ġeverything , Ġit Ġjust Ġwonderful . Ġyou Ġlaugh , Ġyou Ġcry , Ġyou Ġcheer ... Ġit Ġbrings Ġout Ġevery Ġemotion Ġimaginable . Ġit Ġis Ġone Ġof Ġhis Ġfinest Ġpieces Ġof Ġwork Ġ& Ġshould Ġnot Ġbe Ġmissed ! </s>\n",
       "y: CategoryList\n",
       "positive,positive,negative,negative,positive\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0R0HbHcLvWI"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:54:26.664096Z",
     "start_time": "2019-09-04T13:54:26.657203Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kheM2hT1LvWJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_transformers import RobertaModel\n",
    "\n",
    "# defining our model architecture \n",
    "class CustomRobertaModel(nn.Module):\n",
    "    def __init__(self,num_labels=2):\n",
    "        super(CustomRobertaModel,self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels) # defining final output layer\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) # \n",
    "        logits = self.classifier(pooled_output)        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T13:54:37.375403Z",
     "start_time": "2019-09-04T13:54:34.031342Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fAhAsk1cLvWN",
    "outputId": "fa80c2ca-a18e-40b3-abcf-911401f65a92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 473/473 [00:00<00:00, 161608.49B/s]\n",
      "100%|██████████| 501200538/501200538 [00:19<00:00, 25575844.04B/s]\n"
     ]
    }
   ],
   "source": [
    "roberta_model = CustomRobertaModel(num_labels=config.num_labels)\n",
    "\n",
    "learn = Learner(data, roberta_model, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:20:31.006732Z",
     "start_time": "2019-09-02T14:14:45.072892Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "hPoTGAGfLvWT",
    "outputId": "3ba9126a-4721-4886-d5ea-0bf56378462c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.220791</td>\n",
       "      <td>0.155920</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>40:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
    "learn.fit_one_cycle(config.epochs, max_lr=config.max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nHkXmH0B_9oU",
    "outputId": "69900299-33a0-4167-d4ff-6934c65f66ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "aCJuCBvV2moU",
    "outputId": "3b06e43e-b12b-4a52-e3a4-8796bf92ffcc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhiv/j/JA9WjtuuH9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find an appropriate lr\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iU_viYtpLvWX"
   },
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:11:19.976614Z",
     "start_time": "2019-09-02T14:11:19.970844Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eh_7KqcJLvWZ"
   },
   "outputs": [],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    learn.model.roberta.eval()\n",
    "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in data.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    ordered_preds = preds[reverse_sampler, :]\n",
    "    pred_values = np.argmax(ordered_preds, axis=1)\n",
    "    return ordered_preds, pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:11:40.835200Z",
     "start_time": "2019-09-02T14:11:20.102674Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sF4xXNAoLvWh"
   },
   "outputs": [],
   "source": [
    "preds, pred_values = get_preds_as_nparray(DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T14:11:40.840225Z",
     "start_time": "2019-09-02T14:11:40.837056Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uMrBWVdsLvWn",
    "outputId": "1873e4cb-a21a-4151-8311-8c63cb85e030",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.942"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on valid\n",
    "(pred_values == data.valid_ds.y.items).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91dIS2DgAgzX"
   },
   "source": [
    "# SuperGLUE Task CB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTMIWkgdFOvY"
   },
   "source": [
    "[SuperGLUE](https://super.gluebenchmark.com) benchmark представляет задания для тестирования качества моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qrHaSVIFlFr"
   },
   "source": [
    "1. Broadcoverage Diagnostics (AX-b)\n",
    "2. CommitmentBank (CB)\n",
    "3. Choice of Plausible Alternatives (COPA)\n",
    "4. Multi-Sentence Reading Comprehension (MultiRC)\n",
    "5. Recognizing Textual Entailment (RTE)\n",
    "6. Words in Context (WiC)\n",
    "7. The Winograd Schema Challenge (WC)\n",
    "8. BoolQ\n",
    "9. Reading Comprehension with Commonsense Reasoning (ReCoRD)\n",
    "10. Winogender Schema Diagnostics (AX-g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9L0cjMoXGjnG"
   },
   "source": [
    "**CB Task**. CommitmentBank - датасет для задачи textual entailment. Дана предпосылка (premise), нужно определить, является ли предложенная гипотеза (hypothesis) для предпосылки следствием (entailment), противоречит ли гипотеза предпосылке (contradiction), или между ними нет связи (neutral). https://github.com/mcdm/CommitmentBank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UW58S09hR9H0"
   },
   "source": [
    "### Источник\n",
    "https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-superglue-task-cb-c362961be957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_imMWIFS194"
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.metrics import *\n",
    "from pytorch_transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meSmP7tbAfaO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creating a config object to store task specific information\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    task = \"CB\",\n",
    "    testing=False,\n",
    "    seed = 2019,\n",
    "    roberta_model_name='roberta-base', # can also be exchanged with roberta-large \n",
    "    max_lr=1e-5,\n",
    "    epochs=10,\n",
    "    use_fp16=False,\n",
    "    bs=4, \n",
    "    max_seq_len=256, \n",
    "    num_labels = 3,\n",
    "    hidden_dropout_prob=.05,\n",
    "    hidden_size=768, # 1024 for roberta-large\n",
    "    start_tok = \"<s>\",\n",
    "    end_tok = \"</s>\",\n",
    "    mark_fields=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBsmISctUFA4"
   },
   "source": [
    "## CommitmentBank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuYFBkJQUOT_"
   },
   "source": [
    "Скрипт для скачивания данных:\n",
    " https://github.com/nyu-mll/jiant/blob/2d0b19f504ffc6fc489d8a397aa4e29dd22decc6/scripts/download_superglue_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nDZ5rLsyUKmw",
    "outputId": "f5f5fc52-73e7-4e4f-85b8-09091c7baeb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting CB...\n",
      "\tCompleted! Downloaded CB data to directory data\n"
     ]
    }
   ],
   "source": [
    "!python download_superglue_data.py --data_dir data --tasks CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsK8upXOFMIY"
   },
   "outputs": [],
   "source": [
    "path = Path(\".\")\n",
    "data_path = path/\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlS_AiDVTWuT"
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(data_path/config.task/\"train.jsonl\",lines=True)\n",
    "val = pd.read_json(data_path/config.task/\"val.jsonl\",lines=True)\n",
    "test = pd.read_json(data_path/config.task/\"test.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "DU1H82EgTatJ",
    "outputId": "3536204b-4d1f-4a17-ce95-b98be551efc9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It was a complex language. Not written down bu...</td>\n",
       "      <td>the language was peeled down</td>\n",
       "      <td>entailment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is part of their religion, a religion I do ...</td>\n",
       "      <td>no women are allowed to take part in this ritual</td>\n",
       "      <td>entailment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Paris to Rouen railway was being extended ...</td>\n",
       "      <td>Gustave was shepherded into creative retreat a...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Part of it was to be compulsorily purchased. Y...</td>\n",
       "      <td>Gustave was driven to creative retreat in Croi...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some of them, like for instance the farm in Co...</td>\n",
       "      <td>buying places is a hobby</td>\n",
       "      <td>entailment</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Look, my dear, I'm not in my dotage yet, and I...</td>\n",
       "      <td>the only form of comfort he has are his compla...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Then the silence in the Zoo became complete. W...</td>\n",
       "      <td>Woil was afraid</td>\n",
       "      <td>entailment</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>But, of course, that just wasn't possible. Her...</td>\n",
       "      <td>marriage to Jonathan would have been a ghastly...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Like now. The Community in Knockglen would def...</td>\n",
       "      <td>the girl had a point</td>\n",
       "      <td>entailment</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>``They have to be crushed, Bobkins!'' So sayin...</td>\n",
       "      <td>behind the house was a vast garden</td>\n",
       "      <td>entailment</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  ... idx\n",
       "0  It was a complex language. Not written down bu...  ...   0\n",
       "1  It is part of their religion, a religion I do ...  ...   1\n",
       "2  The Paris to Rouen railway was being extended ...  ...   2\n",
       "3  Part of it was to be compulsorily purchased. Y...  ...   3\n",
       "4  Some of them, like for instance the farm in Co...  ...   4\n",
       "5  Look, my dear, I'm not in my dotage yet, and I...  ...   5\n",
       "6  Then the silence in the Zoo became complete. W...  ...   6\n",
       "7  But, of course, that just wasn't possible. Her...  ...   7\n",
       "8  Like now. The Community in Knockglen would def...  ...   8\n",
       "9  ``They have to be crushed, Bobkins!'' So sayin...  ...   9\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqlbIBMWV6d2"
   },
   "outputs": [],
   "source": [
    "# drop the unnecessary idx column\n",
    "for df in (train,val):\n",
    "    if \"idx\" in df.columns: df.drop(\"idx\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "VG8nZ7RWVKVq",
    "outputId": "b31ea4e6-9e7e-46fc-efeb-b1f4eb2c6725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 3)\n",
      "(56, 3)\n",
      "(250, 3)\n"
     ]
    }
   ],
   "source": [
    "if config.testing:\n",
    "    train = train[:100]\n",
    "    val = val[:100]\n",
    "    \n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "JdVEpy8nVvjY",
    "outputId": "e47c303b-00a7-47dc-8c60-42b15b559ec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contradiction    119\n",
       "entailment       115\n",
       "neutral           16\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhdre2XrV1mB"
   },
   "outputs": [],
   "source": [
    "feat_cols = [\"premise\",\"hypothesis\"]\n",
    "label_cols = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iU7r0BBnay7D"
   },
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whmYv_R6c9qz"
   },
   "source": [
    "Специальные символы начала и конца последовательности для разделения нескольких предложений: <br>\n",
    "`<s>` after taking a quick nap, the sheep crossed the street `<s>` `<s>` the sheep took a nap `<s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w00ld2c5WB-F"
   },
   "outputs": [],
   "source": [
    "class FastAiRobertaTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n",
    "    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len \n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self \n",
    "    def tokenizer(self, t:str) -> List[str]: \n",
    "        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n",
    "        if config.mark_fields:\n",
    "            sub = 2 # subtraction in totoal seq_length to be made due to adding spcl tokens\n",
    "            assert \"xxfld\" in t\n",
    "            t = t.replace(\"xxfld 1\",\"\") # remove the xxfld 1 special token from fastai\n",
    "            # converting fastai field sep token to Roberta\n",
    "            t = re.split(r'xxfld \\d+', t) \n",
    "            res = []\n",
    "            for i in range(len(t)-1): # loop over the number of additional fields and the Roberta sep\n",
    "                res += self._pretrained_tokenizer.tokenize(t[i]) + [config.end_tok, config.end_tok]\n",
    "                sub += 2 # increase our subtractions since we added more spcl tokens\n",
    "            res += self._pretrained_tokenizer.tokenize(t[-1]) # add the last sequence\n",
    "            return [config.start_tok] + res[:self.max_seq_len - sub] + [config.end_tok] \n",
    "        \n",
    "        res = self._pretrained_tokenizer.tokenize(t)\n",
    "        return [config.start_tok] + res[:self.max_seq_len - sub] + [config.end_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4lw-P58eUtj"
   },
   "outputs": [],
   "source": [
    "#  create fastai tokenizer for roberta\n",
    "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n",
    "                             pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PtQJuhtJfAYa"
   },
   "source": [
    "Аналогично загружается словарь RoBERTa, создается DataBunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twM51S-CejEZ"
   },
   "outputs": [],
   "source": [
    "# create fastai vocabulary for roberta\n",
    "roberta_tok.save_vocabulary(path)\n",
    "\n",
    "with open('vocab.json', 'r') as f:\n",
    "    roberta_vocab_dict = json.load(f)\n",
    "    \n",
    "fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVImfeQuent9"
   },
   "outputs": [],
   "source": [
    "# Setting up pre-processors\n",
    "class RobertaTokenizeProcessor(TokenizeProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False, mark_fields=config.mark_fields)\n",
    "\n",
    "class RobertaNumericalizeProcessor(NumericalizeProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n",
    "\n",
    "\n",
    "def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    \"\"\"\n",
    "    Constructing preprocessors for Roberta\n",
    "    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n",
    "    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n",
    "    \"\"\"\n",
    "    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1g-zzpzxevs_"
   },
   "outputs": [],
   "source": [
    "# Creating a Roberta specific DataBunch class\n",
    "class RobertaDataBunch(TextDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training Roberta\"\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n",
    "               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n",
    "               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n",
    "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SlUEq9Gc8Bs"
   },
   "outputs": [],
   "source": [
    "# loading the tokenizer and vocab processors\n",
    "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
    "\n",
    "# creating our databunch \n",
    "data = ItemLists(\".\", RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor),\n",
    "                      RobertaTextList.from_df(val, \".\", cols=feat_cols, processor=processor)\n",
    "                ) \\\n",
    "       .label_from_df(cols=label_cols, label_cls=CategoryList) \\\n",
    "       .add_test(RobertaTextList.from_df(test, \".\", cols=feat_cols, processor=processor)) \\\n",
    "       .databunch(bs=4,pad_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypYE_VKQgo-c"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8eouzktg6EX"
   },
   "source": [
    "Здесь используется класс RobertaForSequenceClassification - трансформер Roberta с линейным слоем для классификации последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XD32fKrVdjcV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_transformers import RobertaForSequenceClassification\n",
    "\n",
    "# defining our model architecture \n",
    "class RobertaForSequenceClassificationModel(nn.Module):\n",
    "    def __init__(self,num_labels=config.num_labels):\n",
    "        super(RobertaForSequenceClassificationModel,self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaForSequenceClassification.from_pretrained(config.roberta_model_name,num_labels= self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids, token_type_ids, attention_mask)\n",
    "        logits = outputs[0] \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S65_pT1ohT58"
   },
   "outputs": [],
   "source": [
    "roberta_model = RobertaForSequenceClassificationModel() \n",
    "\n",
    "learn = Learner(data, roberta_model, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "jU1kII0GhXgg",
    "outputId": "9de85416-4ca1-4dfb-da6d-c600b98ec342"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.101404</td>\n",
       "      <td>1.072698</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.990895</td>\n",
       "      <td>0.886807</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.812584</td>\n",
       "      <td>0.763674</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.578803</td>\n",
       "      <td>0.487339</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.379374</td>\n",
       "      <td>0.420436</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.240708</td>\n",
       "      <td>0.333549</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.155583</td>\n",
       "      <td>0.321086</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.108036</td>\n",
       "      <td>0.250627</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.077963</td>\n",
       "      <td>0.266611</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>0.263629</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
    "learn.fit_one_cycle(config.epochs, max_lr=config.max_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HBbY0IEiITk"
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlzS7_eDhn0o"
   },
   "outputs": [],
   "source": [
    "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
    "    learn.model.roberta.eval()\n",
    "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
    "    sampler = [i for i in data.dl(ds_type).sampler]\n",
    "    reverse_sampler = np.argsort(sampler)\n",
    "    ordered_preds = preds[reverse_sampler, :]\n",
    "    pred_values = np.argmax(ordered_preds, axis=1)\n",
    "    return ordered_preds, pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xosBKEjpixlX"
   },
   "outputs": [],
   "source": [
    "# val preds\n",
    "preds, pred_values = get_preds_as_nparray(DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i7ORPDVvizIP",
    "outputId": "41608ad8-e502-4ed5-8f8f-dfad862f70ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8928571428571429"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy for valid \n",
    "(pred_values == data.valid_ds.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZQ468iliPdK"
   },
   "outputs": [],
   "source": [
    "# test preds\n",
    "_, test_pred_values = get_preds_as_nparray(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iWOXWvFQiZWz",
    "outputId": "0a92c9d8-e94e-4c46-df25-0f3c17634142"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.468"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy for test\n",
    "(test_pred_values == data.test_ds.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpN6uqxeii29"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sem_10_fastai.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
